{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#@title Install dependencies\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhashlib\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "#@title Install dependencies\n",
    "import os\n",
    "from google.colab import files\n",
    "import re\n",
    "import hashlib\n",
    "import random\n",
    "\n",
    "from sys import version_info\n",
    "python_version = f\"{version_info.major}.{version_info.minor}\"\n",
    "\n",
    "USE_AMBER = True\n",
    "USE_TEMPLATES = False\n",
    "PYTHON_VERSION = python_version\n",
    "\n",
    "if not os.path.isfile(\"COLABFOLD_READY\"):\n",
    "  print(\"installing colabfold...\")\n",
    "  os.system(\"pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'\")\n",
    "  if os.environ.get('TPU_NAME', False) != False:\n",
    "    os.system(\"pip uninstall -y jax jaxlib\")\n",
    "    os.system(\"pip install --no-warn-conflicts --upgrade dm-haiku==0.0.10 'jax[cuda12_pip]'==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\")\n",
    "  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold\")\n",
    "  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold\")\n",
    "  os.system(\"touch COLABFOLD_READY\")\n",
    "\n",
    "if USE_AMBER or USE_TEMPLATES:\n",
    "  if not os.path.isfile(\"CONDA_READY\"):\n",
    "    print(\"installing conda...\")\n",
    "    os.system(\"wget -qnc https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\")\n",
    "    os.system(\"bash Miniforge3-Linux-x86_64.sh -bfp /usr/local\")\n",
    "    os.system(\"mamba config --set auto_update_conda false\")\n",
    "    os.system(\"touch CONDA_READY\")\n",
    "\n",
    "if USE_TEMPLATES and not os.path.isfile(\"HH_READY\") and USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n",
    "  print(\"installing hhsuite and amber...\")\n",
    "  os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer\")\n",
    "  os.system(\"touch HH_READY\")\n",
    "  os.system(\"touch AMBER_READY\")\n",
    "else:\n",
    "  if USE_TEMPLATES and not os.path.isfile(\"HH_READY\"):\n",
    "    print(\"installing hhsuite...\")\n",
    "    os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python='{PYTHON_VERSION}'\")\n",
    "    os.system(\"touch HH_READY\")\n",
    "  if USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n",
    "    print(\"installing amber...\")\n",
    "    os.system(f\"mamba install -y -c conda-forge openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer\")\n",
    "    os.system(\"touch AMBER_READY\")\n",
    "\n",
    "os.system(\"pip install biopython\")\n",
    "os.system(\"pip install gemmi\")\n",
    "\n",
    "print(\"Downloading RNA3Db...\")\n",
    "# Donwload RNA3Db structure files\n",
    "os.system('wget https://github.com/marcellszi/rna3db/releases/download/incremental-update/rna3db-mmcifs.v2.tar.xz')\n",
    "print(\"Extracting structure files...\")\n",
    "os.system('tar -xf rna3db-mmcifs.v2.tar.xz')\n",
    "os.system('rm rna3db-mmcifs.v2.tar.xz')\n",
    "struct_path = \"/content/rna3db-mmcifs.v2/rna3db-mmcifs\"\n",
    "\n",
    "# Donwload RNA3Db sequence files\n",
    "os.system('wget https://github.com/marcellszi/rna3db/releases/download/incremental-update/rna3db-jsons.tar.gz')\n",
    "print(\"Extracting sequence files...\")\n",
    "os.system('tar -xzf rna3db-jsons.tar.gz')\n",
    "os.system('rm rna3db-jsons.tar.gz')\n",
    "seq_path = \"/content/rna3db-jsons/split.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hash(x,y):\n",
    "  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class Converter(nn.Module):\n",
    "    def __init__(self, max_seq_len, d_model=64, nhead=8, num_layers=6, dim_feedforward=256, dropout=0.1):\n",
    "        super(Converter, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.input_embedding = nn.Linear(1, d_model)\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len=max_seq_len)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, \n",
    "                                                    dim_feedforward=dim_feedforward, \n",
    "                                                    dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        \n",
    "        self.output_linear = nn.Linear(d_model, 20)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        # x shape: (seq_len, batch_size, 1)\n",
    "        \n",
    "        x = self.input_embedding(x)  # Now: (seq_len, batch_size, d_model)\n",
    "        \n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        x = self.output_linear(x)  # Now: (seq_len, batch_size, 20)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        # Convert softmaxxed matrices into one-dimensional indeces\n",
    "        with torch.no_grad():\n",
    "            out = []\n",
    "            for i in range(len(x)):\n",
    "                out.append([])\n",
    "                for j in range(len(x[i])):\n",
    "                    out[-1].append((torch.argmax(x[i][j].detach().cpu())).item()) \n",
    "        return out\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def create_padding_mask(sequences, pad_value=0):\n",
    "    # sequences shape: (seq_len, batch_size, 1)\n",
    "    return (sequences.squeeze(-1) == pad_value).t()  # (batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "def parse_json(path, a, b, max_len=150):\n",
    "    num = -1\n",
    "    seqs = {}\n",
    "    comps = []\n",
    "    macros = []\n",
    "    f = open(path)\n",
    "    data = json.load(f)\n",
    "    for i in data[\"train_set\"]:\n",
    "        for j in data[\"train_set\"][i]:\n",
    "            for k in data[\"train_set\"][i][j]:\n",
    "                num = num + 1\n",
    "                if data[\"train_set\"][i][j][k][\"length\"]>max_len:\n",
    "                    continue\n",
    "                if num>=a and num<=b:\n",
    "                    seqs[k]=data[\"train_set\"][i][j][k][\"sequence\"]\n",
    "                    comps.append(i)\n",
    "                    macros.append(j)\n",
    "                if num>b:\n",
    "                    break\n",
    "    f.close()\n",
    "    return seqs, comps, macros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = {} # All sequences - may get quite large\n",
    "\n",
    "# Used for file tree searching\n",
    "components = []\n",
    "macro_tags = []\n",
    "\n",
    "\n",
    "# Index to amino acid dictionary\n",
    "# Largely arbitrary, but must stay consistent for any given converter\n",
    "AA_DICT = {\n",
    "    0: \"A\",\n",
    "    1: \"R\",\n",
    "    2: \"N\",\n",
    "    3: \"D\",\n",
    "    4: \"C\",\n",
    "    5: \"Q\",\n",
    "    6: \"E\",\n",
    "    7: \"G\",\n",
    "    8: \"H\",\n",
    "    9: \"I\",\n",
    "    10: \"L\",\n",
    "    11: \"K\",\n",
    "    12: \"M\",\n",
    "    13: \"F\",\n",
    "    14: \"P\",\n",
    "    15: \"S\",\n",
    "    16: \"T\",\n",
    "    17: \"W\",\n",
    "    18: \"Y\",\n",
    "    19: \"V\"\n",
    "}\n",
    "\n",
    "def load_data(path, a=0, b=float('inf'), max_len=150):\n",
    "    # Load up sequences, components, and macro-tags\n",
    "    seqs, components, macro_tags=parse_json(path, a, b, max_len=max_len)\n",
    "    return seqs, components, macro_tags\n",
    "\n",
    "def batch_data(iterable, n=1):\n",
    "    # Data batching function\n",
    "    l = len(iterable)\n",
    "    iter = [(t, s) for t, s in list(iterable.items())]\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iter[ndx:min(ndx + n, l)]\n",
    "\n",
    "def encode_rna(seq):\n",
    "    # Convert RNA sequence to nums to feed into Converter\n",
    "    out = []\n",
    "    for i in seq:\n",
    "        if i==\"A\":\n",
    "            out.append([0])\n",
    "        elif i==\"U\":\n",
    "            out.append([1])\n",
    "        elif i==\"C\":\n",
    "            out.append([2])\n",
    "        elif i==\"G\":\n",
    "            out.append([3])\n",
    "    return out\n",
    "\n",
    "def write_fastas(seqs):\n",
    "    # Write a dict of {tag: seq} to as many FASTA files as needed\n",
    "    os.makedirs('FASTAs', exist_ok=True)\n",
    "    for tag, seq in list(seqs.items()):\n",
    "        if os.path.exists(f'/content/FASTAs/{tag}.fasta'):\n",
    "            continue\n",
    "        f = open(f\"/content/FASTAs/{tag}.fasta\", \"w+\")\n",
    "        f.write(f\">{tag}\\n{seq}\")\n",
    "        f.close()\n",
    "\n",
    "def write_fastas(tags, seqs):\n",
    "    # Write a dict of {tag: seq} to as many FASTA files as needed\n",
    "    os.makedirs('FASTAs', exist_ok=True)\n",
    "    for i in range(len(tags)):\n",
    "        if os.path.exists(f'/content/FASTAs/{tags[i]}.fasta'):\n",
    "            continue\n",
    "        f = open(f\"/content/FASTAs/{tags[i]}.fasta\", \"w+\")\n",
    "        f.write(f\">{tags[i]}\\n{seqs[i]}\")\n",
    "        f.close()\n",
    "\n",
    "def empty_dir(path):\n",
    "    # Empty any directory\n",
    "    for f in os.listdir(path):\n",
    "        os.remove(os.path.join(path, f))\n",
    "    os.rmdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_structure(tag, path):\n",
    "    # Return the structure of an RNA molecule given its tag and the path to the structure directory\n",
    "    # File directory:\n",
    "    # root\n",
    "    #  -- colabfold.dir\n",
    "    #  -- train_protify.ipynb\n",
    "    #  -- data.dir\n",
    "    #  ---- component 1.dir\n",
    "    #  ------ tag 1.dir\n",
    "    #  -------- tag 1a.cif\n",
    "    #  -------- tag 1b.cif\n",
    "    # ...\n",
    "    index = list(seqs.keys()).index(tag)\n",
    "    component = components[index]\n",
    "    macro_tag = macro_tags[index]\n",
    "    \n",
    "    path = f\"{path}\\{component}\\{macro_tag}\\{tag}.cif\"\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### Advanced settings\n",
    "model_type = \"auto\" #@param [\"auto\", \"alphafold2_ptm\", \"alphafold2_multimer_v1\", \"alphafold2_multimer_v2\", \"alphafold2_multimer_v3\", \"deepfold_v1\"]\n",
    "#@markdown - if `auto` selected, will use `alphafold2_ptm` for monomer prediction and `alphafold2_multimer_v3` for complex prediction.\n",
    "#@markdown Any of the mode_types can be used (regardless if input is monomer or complex).\n",
    "num_recycles = \"3\" #@param [\"auto\", \"0\", \"1\", \"3\", \"6\", \"12\", \"24\", \"48\"]\n",
    "#@markdown - if `auto` selected, will use `num_recycles=20` if `model_type=alphafold2_multimer_v3`, else `num_recycles=3` .\n",
    "recycle_early_stop_tolerance = \"auto\" #@param [\"auto\", \"0.0\", \"0.5\", \"1.0\"]\n",
    "#@markdown - if `auto` selected, will use `tol=0.5` if `model_type=alphafold2_multimer_v3` else `tol=0.0`.\n",
    "relax_max_iterations = 200 #@param [0, 200, 2000] {type:\"raw\"}\n",
    "#@markdown - max amber relax iterations, `0` = unlimited (AlphaFold2 default, can take very long)\n",
    "pairing_strategy = \"greedy\" #@param [\"greedy\", \"complete\"] {type:\"string\"}\n",
    "#@markdown - `greedy` = pair any taxonomically matching subsets, `complete` = all sequences have to match in one line.\n",
    "\n",
    "\n",
    "#@markdown #### Sample settings\n",
    "#@markdown -  enable dropouts and increase number of seeds to sample predictions from uncertainty of the model.\n",
    "#@markdown -  decrease `max_msa` to increase uncertainity\n",
    "max_msa = \"auto\" #@param [\"auto\", \"512:1024\", \"256:512\", \"64:128\", \"32:64\", \"16:32\"]\n",
    "num_seeds = 1 #@param [1,2,4,8,16] {type:\"raw\"}\n",
    "use_dropout = False #@param {type:\"boolean\"}\n",
    "\n",
    "num_recycles = None if num_recycles == \"auto\" else int(num_recycles)\n",
    "recycle_early_stop_tolerance = None if recycle_early_stop_tolerance == \"auto\" else float(recycle_early_stop_tolerance)\n",
    "if max_msa == \"auto\": max_msa = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### MSA options (custom MSA upload, single sequence, pairing mode)\n",
    "msa_mode = \"mmseqs2_uniref_env\" #@param [\"mmseqs2_uniref_env\", \"mmseqs2_uniref\",\"single_sequence\",\"custom\"]\n",
    "pair_mode = \"unpaired_paired\" #@param [\"unpaired_paired\",\"paired\",\"unpaired\"] {type:\"string\"}\n",
    "#@markdown - \"unpaired_paired\" = pair sequences from same species + unpaired MSA, \"unpaired\" = seperate MSA for each chain, \"paired\" - only use paired sequences.\n",
    "\n",
    "def a3ms(jobname):\n",
    "    # decide which a3m to use\n",
    "    if \"mmseqs2\" in msa_mode:\n",
    "        a3m_file = os.path.join(jobname,f\"{jobname}.a3m\")\n",
    "\n",
    "    elif msa_mode == \"custom\":\n",
    "        a3m_file = os.path.join(jobname,f\"{jobname}.custom.a3m\")\n",
    "        if not os.path.isfile(a3m_file):\n",
    "            custom_msa_dict = files.upload()\n",
    "            custom_msa = list(custom_msa_dict.keys())[0]\n",
    "            header = 0\n",
    "            import fileinput\n",
    "            for line in fileinput.FileInput(custom_msa,inplace=1):\n",
    "                if line.startswith(\">\"):\n",
    "                    header = header + 1\n",
    "                if not line.rstrip():\n",
    "                    continue\n",
    "                if line.startswith(\">\") == False and header == 1:\n",
    "                    query_sequence = line.rstrip()\n",
    "                print(line, end='')\n",
    "\n",
    "            os.rename(custom_msa, a3m_file)\n",
    "            queries_path=a3m_file\n",
    "            print(f\"moving {custom_msa} to {a3m_file}\")\n",
    "\n",
    "    else:\n",
    "        a3m_file = os.path.join(jobname,f\"{jobname}.single_sequence.a3m\")\n",
    "        with open(a3m_file, \"w\") as text_file:\n",
    "            text_file.write(\">1\\n%s\" % query_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from gemmi import *\n",
    "\n",
    "class Monomer:\n",
    "    def __init__(self, macro):\n",
    "        self.atoms = dict()\n",
    "        self.name = \"\"\n",
    "        self.macro = macro\n",
    "        \n",
    "    def add_atom(self, x, y, z, e):\n",
    "        self.atoms[e] = np.array([x, y, z])\n",
    "        \n",
    "    def element(self, e):\n",
    "        if e[0]==\"\\\"\":\n",
    "            return e[1]\n",
    "        else:\n",
    "            return e[0]\n",
    "        \n",
    "    def get_atoms(self):\n",
    "        return self.atoms\n",
    "    \n",
    "    def add_name(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def apply_transformation(self, x, y, z):\n",
    "        out = self\n",
    "        for atom in out.atoms:\n",
    "            out.atoms[atom] += [x,y,z]\n",
    "        return out\n",
    "    \n",
    "    def calculate_normal(self):\n",
    "        # Get the triangle vertices\n",
    "        c4_pos = np.array(self.atoms['\"C4\\'\"'])\n",
    "        c1_pos = np.array(self.atoms['\"C1\\'\"'])\n",
    "        translation = -np.array(self.atoms['P'])\n",
    "            \n",
    "        # Calculate triangle vectors\n",
    "        p_to_c4 = c4_pos + translation  # Vector from P to C4'\n",
    "        p_to_c1 = c1_pos + translation  # Vector from P to C1'\n",
    "        \n",
    "        # Calculate normal to triangle\n",
    "        normal = np.cross(p_to_c4, p_to_c1)\n",
    "        normal = normal / np.linalg.norm(normal)\n",
    "        return normal\n",
    "    \n",
    "    def align_triangle_to_xy(self):\n",
    "        \"\"\"\n",
    "        Aligns the triangle formed by C4', C1', and N1/N9 atoms to the positive xy plane.\n",
    "        \"\"\"\n",
    "        out = copy.deepcopy(self)\n",
    "        # Get the coordinates of the three atoms forming the triangle\n",
    "        c4_coords = np.array(out.get_atom_coordinates('\\\"C4\\'\\\"'))\n",
    "        c1_coords = np.array(out.get_atom_coordinates('\\\"C1\\'\\\"'))\n",
    "        base_coords = np.array(out.get_atom_coordinates(\"P\"))\n",
    "\n",
    "        if c4_coords is None or c1_coords is None or base_coords is None:\n",
    "            raise ValueError(\"Could not find required atoms for alignment\")\n",
    "\n",
    "        # Create vectors from C4' to C1' and C4' to N1/N9\n",
    "        v1 = c1_coords - c4_coords\n",
    "        v2 = base_coords - c4_coords\n",
    "\n",
    "        # Calculate the normal vector of the triangle\n",
    "        normal = np.cross(v1, v2)\n",
    "        normal_magnitude = np.linalg.norm(normal)\n",
    "        \n",
    "        if normal_magnitude < 1e-10:\n",
    "            raise ValueError(\"Colinear points cannot form a triangle\")\n",
    "            \n",
    "        normal = normal / normal_magnitude\n",
    "\n",
    "        # Calculate rotation matrix to align normal vector with z-axis\n",
    "        z_axis = np.array([0, 0, 1])\n",
    "        rotation_axis = np.cross(normal, z_axis)\n",
    "        rotation_axis_magnitude = np.linalg.norm(rotation_axis)\n",
    "\n",
    "        if rotation_axis_magnitude < 1e-10:\n",
    "            # If vectors are parallel, no rotation needed or rotate 180째 if antiparallel\n",
    "            if normal[2] < 0:\n",
    "                # If normal points in negative z, rotate 180째 around x-axis\n",
    "                rotation_matrix = np.array([\n",
    "                    [1, 0, 0],\n",
    "                    [0, -1, 0],\n",
    "                    [0, 0, -1]\n",
    "                ])\n",
    "            else:\n",
    "                return  # Already aligned correctly\n",
    "        else:\n",
    "            rotation_axis = rotation_axis / rotation_axis_magnitude\n",
    "            angle = np.arccos(np.clip(np.dot(normal, z_axis), -1.0, 1.0))\n",
    "            \n",
    "            # Create rotation matrix using Rodrigues' rotation formula\n",
    "            K = np.array([\n",
    "                [0, -rotation_axis[2], rotation_axis[1]],\n",
    "                [rotation_axis[2], 0, -rotation_axis[0]],\n",
    "                [-rotation_axis[1], rotation_axis[0], 0]\n",
    "            ])\n",
    "            rotation_matrix = (np.eye(3) + np.sin(angle) * K + \n",
    "                            (1 - np.cos(angle)) * np.matmul(K, K))\n",
    "\n",
    "        # Apply rotation to all atoms\n",
    "        for atom in out.atoms.keys():\n",
    "            coords = np.array(out.atoms[atom]) - c4_coords  # Center around C4'\n",
    "            rotated_coords = np.dot(rotation_matrix, coords)\n",
    "            atom.set_coordinates(rotated_coords + c4_coords)  # Move back to original position\n",
    "\n",
    "        # After first rotation, calculate the angle in xy plane between C4'-C1' vector and x-axis\n",
    "        c4_coords = np.array(out.get_atom_coordinates('C4\\''))\n",
    "        c1_coords = np.array(out.get_atom_coordinates('C1\\''))\n",
    "        v1_xy = c1_coords[:2] - c4_coords[:2]  # Only consider x and y components\n",
    "        v1_xy_magnitude = np.linalg.norm(v1_xy)\n",
    "        \n",
    "        if v1_xy_magnitude < 1e-10:\n",
    "            return  # Vector is vertical, no need for xy rotation\n",
    "            \n",
    "        cos_theta = np.clip(np.dot(v1_xy, [1, 0]) / v1_xy_magnitude, -1.0, 1.0)\n",
    "        theta = np.arccos(cos_theta)\n",
    "        \n",
    "        # Determine if we need to rotate clockwise or counterclockwise\n",
    "        if v1_xy[1] < 0:\n",
    "            theta = -theta\n",
    "\n",
    "        # Create rotation matrix around z-axis\n",
    "        rotation_matrix_z = np.array([\n",
    "            [np.cos(theta), -np.sin(theta), 0],\n",
    "            [np.sin(theta), np.cos(theta), 0],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "\n",
    "        # Apply second rotation to all atoms\n",
    "        for atom in out.atoms:\n",
    "            coords = np.array(atom.get_coordinates()) - c4_coords\n",
    "            rotated_coords = np.dot(rotation_matrix_z, coords)\n",
    "            atom.set_coordinates(rotated_coords + c4_coords)\n",
    "\n",
    "        # Final check to ensure the molecule is in the positive xy plane\n",
    "        # If the base atom is in the negative x region, rotate 180째 around y-axis\n",
    "        base_coords = np.array(out.get_atom_coordinates(out.base_atom))\n",
    "        if base_coords[0] - c4_coords[0] < 0:\n",
    "            rotation_matrix_y = np.array([\n",
    "                [-1, 0, 0],\n",
    "                [0, 1, 0],\n",
    "                [0, 0, -1]\n",
    "            ])\n",
    "            for atom in out.atoms:\n",
    "                coords = np.array(atom.get_coordinates()) - c4_coords\n",
    "                rotated_coords = np.dot(rotation_matrix_y, coords)\n",
    "                atom.set_coordinates(rotated_coords + c4_coords)\n",
    "\n",
    "\n",
    "    def align_to_normal(self, target_normal):\n",
    "        \"\"\"\n",
    "        Rotates the monomer so that the normal vector of its P-C1'-C4' triangle \n",
    "        aligns with the given target normal vector.\n",
    "        \n",
    "        Args:\n",
    "            target_normal (np.ndarray): The target normal vector to align with (should be normalized)\n",
    "            \n",
    "        Returns:\n",
    "            Monomer: A new Monomer instance with the rotated coordinates\n",
    "        \"\"\"\n",
    "        out = copy.deepcopy(self)\n",
    "        \n",
    "        try:\n",
    "            # Get current normal vector\n",
    "            current_normal = self.calculate_normal()\n",
    "            \n",
    "            # Normalize target vector\n",
    "            target_normal = target_normal / np.linalg.norm(target_normal)\n",
    "            \n",
    "            # Calculate rotation axis and angle\n",
    "            rotation_axis = np.cross(current_normal, target_normal)\n",
    "            \n",
    "            # If vectors are parallel (or anti-parallel), rotation axis will be zero\n",
    "            if np.linalg.norm(rotation_axis) < 1e-10:\n",
    "                # If normals are anti-parallel, rotate 180째 around any perpendicular axis\n",
    "                if np.dot(current_normal, target_normal) < 0:\n",
    "                    # Find a perpendicular vector to rotate around\n",
    "                    if abs(current_normal[0]) < abs(current_normal[1]):\n",
    "                        rotation_axis = np.cross(current_normal, [1, 0, 0])\n",
    "                    else:\n",
    "                        rotation_axis = np.cross(current_normal, [0, 1, 0])\n",
    "                    angle = np.pi\n",
    "                else:\n",
    "                    # Vectors are already aligned\n",
    "                    return out\n",
    "            else:\n",
    "                # Calculate rotation angle\n",
    "                angle = np.arccos(np.clip(np.dot(current_normal, target_normal), -1.0, 1.0))\n",
    "            \n",
    "            # Normalize rotation axis\n",
    "            rotation_axis = rotation_axis / np.linalg.norm(rotation_axis)\n",
    "            \n",
    "            # Create rotation matrix using Rodrigues' rotation formula\n",
    "            cos_theta = np.cos(angle)\n",
    "            sin_theta = np.sin(angle)\n",
    "            K = np.array([\n",
    "                [0, -rotation_axis[2], rotation_axis[1]],\n",
    "                [rotation_axis[2], 0, -rotation_axis[0]],\n",
    "                [-rotation_axis[1], rotation_axis[0], 0]\n",
    "            ])\n",
    "            R = np.eye(3) + sin_theta * K + (1 - cos_theta) * np.dot(K, K)\n",
    "            \n",
    "            # Apply rotation to all atoms\n",
    "            for atom in out.atoms:\n",
    "                out.atoms[atom] = np.dot(R, out.atoms[atom])\n",
    "                \n",
    "            return out\n",
    "            \n",
    "        except KeyError as e:\n",
    "            raise KeyError(f\"Required atom {e} not found in this monomer\")\n",
    "\n",
    "    \n",
    "    def load_template(self, n):\n",
    "        if n==\"A\": path = \"templates/Adenine_template.cif\"\n",
    "        elif n==\"C\": path = \"templates/Cytosine_template.cif\"\n",
    "        elif n==\"G\": path = \"templates/Guanine_template.cif\"\n",
    "        elif n==\"U\": path = \"templates/Uracil_template.cif\"\n",
    "        atoms = []\n",
    "        atom_xs = []\n",
    "        atom_ys = []\n",
    "        atom_zs = []\n",
    "        try:\n",
    "            doc = cif.read_file(path)  # copy all the data from mmCIF file\n",
    "            block = doc.sole_block()  # mmCIF has exactly one block\n",
    "            for element in block.find_loop(\"_atom_site.label_atom_id\"):\n",
    "                atoms.append(element)\n",
    "            for element in block.find_loop(\"_atom_site.Cartn_x\"):\n",
    "                atom_xs.append(float(element))\n",
    "            for element in block.find_loop(\"_atom_site.Cartn_y\"):\n",
    "                atom_ys.append(float(element))\n",
    "            for element in block.find_loop(\"_atom_site.Cartn_z\"):\n",
    "                atom_zs.append(float(element))\n",
    "        except Exception as e:\n",
    "            print(\"Oops. %s\" % e)\n",
    "        for i in range(len(atoms)):\n",
    "            self.add_atom(atom_xs[i], atom_ys[i], atom_zs[i], atoms[i])\n",
    "        \n",
    "        \n",
    "    def __str__(self, start=1):\n",
    "        #Return what this monomer would look like in an mmCIF file#\n",
    "        out = \"\"\n",
    "        c = start\n",
    "        for i in self.atoms:\n",
    "            out += f\"ATOM {c}\\t{self.element(i)}\\t{i}\\t{self.name}\\t. A 1 1\\t?\\t{round(self.atoms[i][0],3)}\\t{round(self.atoms[i][1],3)}\\t{round(self.atoms[i][2],3)}\\n\"\n",
    "            c += 1\n",
    "        out += \"\\b\\b\"\n",
    "        return out\n",
    "    \n",
    "    def print(self, start=1, number=1):\n",
    "        #Return what this monomer would look like in an mmCIF file#\n",
    "        out = \"\"\n",
    "        c = start\n",
    "        for i in self.atoms:\n",
    "            out += f\"ATOM {c}\\t{self.element(i)}\\t{i}\\t{self.name}\\t. A 1 {number}\\t?\\t{round(self.atoms[i][0],3)}\\t{round(self.atoms[i][1],3)}\\t{round(self.atoms[i][2],3)}\\n\"\n",
    "            c += 1\n",
    "        return out\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSD(p1, p2):\n",
    "    loss = torch.sqrt(torch.mean(p1[:len(p2)], p2)**2)\n",
    "    return loss\n",
    "\n",
    "def tm_score(p1, p2, lt):\n",
    "    d0 = lambda l: 1.24 * np.cbrt(l-15) - 1.8\n",
    "    loss = torch.mean(1/(1+np.power(np.abs(np.linalg.norm(p1-p2))/d0(lt),2)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_rna(path):\n",
    "    try:\n",
    "        doc = cif.read_file(path)\n",
    "        block = doc.sole_block()\n",
    "        all_xs = [element for element in block.find_loop(\"_atom_site.Cartn_x\")]\n",
    "        all_ys = [element for element in block.find_loop(\"_atom_site.Cartn_y\")]\n",
    "        all_zs = [element for element in block.find_loop(\"_atom_site.Cartn_z\")]\n",
    "        all_atoms = [element for element in block.find_loop(\"_atom_site.label_atom_id\")]\n",
    "        \n",
    "        points = []\n",
    "        angle_points = []\n",
    "        norms = []\n",
    "        \n",
    "        for x, y, z, atom in zip(all_xs, all_ys, all_zs, all_atoms):\n",
    "            x = float(x)\n",
    "            y = float(y)\n",
    "            z = float(z)\n",
    "\n",
    "            if atom == \"P\":\n",
    "                points.append(np.array([x, y, z]))\n",
    "                angle_points.append(np.array([x, y, z]))\n",
    "            elif atom == \"\\\"C1'\\\"\":\n",
    "                angle_points.append(np.array([x, y, z]))\n",
    "            elif atom == \"\\\"C4'\\\"\":\n",
    "                angle_points.append(np.array([x, y, z]))\n",
    "                v1 = angle_points[-1]-angle_points[-2]\n",
    "                v2 = angle_points[-3]-angle_points[-2]\n",
    "                norms.append(np.cross(v1, v2))\n",
    "                angle_points = []\n",
    "        \n",
    "        return torch.Tensor(points), torch.Tensor(norms)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"Oops. %s\" % e)\n",
    "        sys.exit(1)\n",
    "\n",
    "def parse_protein(path):\n",
    "    try:\n",
    "        doc = cif.read_file(path)\n",
    "        block = doc.sole_block()\n",
    "        all_xs = [element for element in block.find_loop(\"_atom_site.Cartn_x\")]\n",
    "        all_ys = [element for element in block.find_loop(\"_atom_site.Cartn_y\")]\n",
    "        all_zs = [element for element in block.find_loop(\"_atom_site.Cartn_z\")]\n",
    "        all_atoms = [element for element in block.find_loop(\"_atom_site.label_atom_id\")]\n",
    "        \n",
    "        points = []\n",
    "        angle_points = []\n",
    "        norms = []\n",
    "        \n",
    "        for x, y, z, atom in zip(all_xs, all_ys, all_zs, all_atoms):\n",
    "            x = float(x)\n",
    "            y = float(y)\n",
    "            z = float(z)\n",
    "\n",
    "            if atom == \"CA\":\n",
    "                points.append(np.array([x, y, z]))\n",
    "                angle_points.append(np.array([x, y, z]))\n",
    "            elif atom == \"N\":\n",
    "                angle_points.append(np.array([x, y, z]))\n",
    "            elif atom == \"C\":\n",
    "                angle_points.append(np.array([x, y, z]))\n",
    "                v1 = angle_points[-1]-angle_points[-2]\n",
    "                v2 = angle_points[-3]-angle_points[-2]\n",
    "                norms.append(np.cross(v1, v2))\n",
    "                angle_points = []\n",
    "        \n",
    "        return torch.Tensor(points), torch.Tensor(norms)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"Oops. %s\" % e)\n",
    "        sys.exit(1)\n",
    "\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Oops. {e}\")\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protein_to_rna(protein, rna_path, tm=False):\n",
    "    prot_points, _ = parse_protein(protein)\n",
    "    rna_points, _ = parse_rna(rna_path)\n",
    "    prot_points = correct_protein_coords(prot_points, len(prot_points))\n",
    "    if tm:\n",
    "        return tm_score(prot_points, rna_points)\n",
    "    else:\n",
    "        return RMSD(prot_points, rna_points)\n",
    "    \n",
    "def correct_protein_coords(points, points_length, correction_factor=np.array([0,0,0]), new_pts=np.array([])):\n",
    "    # Apply correction factor to the protein coordinates to account for bond lengths\n",
    "    pp_dist = 6.8 # Approximated this value from what ChatGPT tells me - will look for rigorous results\n",
    "                  # Also haha pp\n",
    "    if len(new_pts)==points_length:\n",
    "        return new_pts\n",
    "    v = np.array(points[1])-np.array(points[0]) # Vector between two points\n",
    "    v = correction_factor + (np.linalg.norm(v)-pp_dist)*v/np.linalg.norm(v) # Delta correction factor\n",
    "    new = new_pts\n",
    "    if correction_factor==np.array([0,0,0]):\n",
    "        new.append(np.array(points[0])) # Add first point on the outermost function call\n",
    "    new.append(np.array(points[1])-v) # Apply correction factor to next point and add\n",
    "    return correct_protein_coords(points[1:], points_length, v, new) # Recursive call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from Bio import BiopythonDeprecationWarning\n",
    "warnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)\n",
    "from pathlib import Path\n",
    "from colabfold.download import download_alphafold_params, default_data_dir\n",
    "from colabfold.utils import setup_logging\n",
    "from colabfold.batch import get_queries, run, set_model_type\n",
    "from colabfold.plot import plot_msa_v2\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "def input_features_callback(input_features):\n",
    "  pass\n",
    "\n",
    "def prediction_callback(protein_obj, length,\n",
    "                        prediction_result, input_features, mode):\n",
    "  model_name, relaxed = mode\n",
    "  pass\n",
    "\n",
    "def train(seqs, epochs=50, batch_size=32,tm_score=False, max_seq_len=150):\n",
    "    drive.mount('/content/drive')\n",
    "    !mkdir -p \"/content/drive/My Drive/ConverterWeights\"\n",
    "    try:\n",
    "        K80_chk = os.popen('nvidia-smi | grep \"Tesla K80\" | wc -l').read()\n",
    "    except:\n",
    "        K80_chk = \"0\"\n",
    "        pass\n",
    "    if \"1\" in K80_chk:\n",
    "        print(\"WARNING: found GPU Tesla K80: limited to total length < 1000\")\n",
    "        if \"TF_FORCE_UNIFIED_MEMORY\" in os.environ:\n",
    "            del os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]\n",
    "        if \"XLA_PYTHON_CLIENT_MEM_FRACTION\" in os.environ:\n",
    "            del os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]\n",
    "\n",
    "    # For some reason we need that to get pdbfixer to import\n",
    "    if f\"/usr/local/lib/python{python_version}/site-packages/\" not in sys.path:\n",
    "        sys.path.insert(0, f\"/usr/local/lib/python{python_version}/site-packages/\")\n",
    "\n",
    "    conv = Converter(max_seq_len=max_seq_len)\n",
    "    conv.train()\n",
    "    optimizer = torch.optim.Adam(conv.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "    model_type = set_model_type(False, \"auto\")\n",
    "    download_alphafold_params(model_type, Path(\".\"))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in batch_data(seqs, batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            # batch: ([(tag, seq), (tag, seq),...])\n",
    "\n",
    "            # LAYER 1: RNA-AMINO CONVERSION\n",
    "            tags = [s[0] for s in batch]\n",
    "            structs = [get_structure(tags[i]) for i in range(len(tags))]\n",
    "\n",
    "            # Check that structure files exist\n",
    "            # if not os.path.isfile(get_structure(tags[0])):\n",
    "            #     continue\n",
    "            \n",
    "            # Preprocessing sequences\n",
    "            processed_seqs = [torch.Tensor(np.transpose(np.array(encode_rna(s[1])), (0,1))) for s in batch] # (batch, seq, base)\n",
    "\n",
    "            # Send sequences through the converter\n",
    "            aa_seqs = [conv(s) for s in processed_seqs][0] # (seq, batch, aa)\n",
    "            temp = []\n",
    "            \n",
    "            # Reconvert to letter representation\n",
    "            for i in range(len(aa_seqs)):\n",
    "                temp.append(''.join([AA_DICT[n] for n in aa_seqs[i]]))\n",
    "                    \n",
    "            aa_seqs = temp # (seq: String, batch)\n",
    "\n",
    "            final_seqs = {} # {tag: seq}\n",
    "            for i in range(len(tags)):\n",
    "                final_seqs[tags[i]] = aa_seqs[i]\n",
    "            write_fastas(final_seqs)\n",
    "\n",
    "            num_relax = 0 #@param [0, 1, 5] {type:\"raw\"}\n",
    "            #@markdown - specify how many of the top ranked structures to relax using amber\n",
    "            template_mode = \"none\" #@param [\"none\", \"pdb100\",\"custom\"]\n",
    "            #@markdown - `none` = no template information is used. `pdb100` = detect templates in pdb100 (see [notes](#pdb100)). `custom` - upload and search own templates (PDB or mmCIF format, see [notes](#custom_templates))\n",
    "\n",
    "            use_amber = num_relax > 0\n",
    "            use_cluster_profile = True\n",
    "\n",
    "            if template_mode == \"pdb100\":\n",
    "                use_templates = True\n",
    "                custom_template_path = None\n",
    "            elif template_mode == \"custom\":\n",
    "                custom_template_path = os.path.join(jobname,f\"template\")\n",
    "                os.makedirs(custom_template_path, exist_ok=True)\n",
    "                uploaded = files.upload()\n",
    "                use_templates = True\n",
    "            for fn in uploaded.keys():\n",
    "                os.rename(fn,os.path.join(custom_template_path,fn))\n",
    "            else:\n",
    "                custom_template_path = None\n",
    "                use_templates = False\n",
    "\n",
    "            for i in range(len(final_seqs)):\n",
    "                queries, _ = get_queries(f'/content/FASTAs/{list(final_seqs.keys())[i]}')\n",
    "                jobname = hash(list(final_seqs.keys())[i])\n",
    "                results =  run(\n",
    "                    queries=queries,\n",
    "                    result_dir=jobname,\n",
    "                    use_templates=USE_TEMPLATES,\n",
    "                    custom_template_path=None,\n",
    "                    num_relax=num_relax,\n",
    "                    msa_mode=msa_mode,\n",
    "                    model_type=model_type,\n",
    "                    num_models=1,\n",
    "                    num_recycles=num_recycles,\n",
    "                    relax_max_iterations=relax_max_iterations,\n",
    "                    recycle_early_stop_tolerance=recycle_early_stop_tolerance,\n",
    "                    num_seeds=num_seeds,\n",
    "                    use_dropout=use_dropout,\n",
    "                    model_order=[1,2,3,4,5],\n",
    "                    is_complex=False,\n",
    "                    data_dir=Path(\".\"),\n",
    "                    keep_existing_results=False,\n",
    "                    rank_by=\"auto\",\n",
    "                    pair_mode=pair_mode,\n",
    "                    pairing_strategy=pairing_strategy,\n",
    "                    stop_at_score=float(100),\n",
    "                    prediction_callback=prediction_callback,\n",
    "                    dpi=200,\n",
    "                    zip_results=False,\n",
    "                    save_all=False,\n",
    "                    max_msa=max_msa,\n",
    "                    use_cluster_profile=use_cluster_profile,\n",
    "                    input_features_callback=input_features_callback,\n",
    "                    save_recycles=False,\n",
    "                    user_agent=\"colabfold/google-colab-main\",\n",
    "                )\n",
    "            \n",
    "            path = \"\"\n",
    "            for file in os.listdir(f\"/content/{jobname}\"):\n",
    "                if file.endswith(\".pdb\"):\n",
    "                    path = os.path.join(f\"/{jobname}\", file)\n",
    "                    break\n",
    "            loss = protein_to_rna(path, get_structure(list(final_seqs.keys())[i], struct_path), tm=tm_score)\n",
    "            empty_dir(f\"/{jobname}\")\n",
    "            loss = torch.Tensor([loss])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        torch.save(conv, f'/content/drive/My Drive/ConverterWeights/converter_epoch_{epoch}.pt')\n",
    "        torch.save(conv.state_dict, f'/content/drive/My Drive/ConverterWeights/converter_params_epoch_{epoch}.pt')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs, components, macro_tags = load_data(seq_path, 0, 1614)\n",
    "train(seqs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
