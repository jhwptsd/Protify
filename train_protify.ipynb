{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_0pUGU74rXxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e51e53-c612-40d9-88e1-1705c97516ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "installing colabfold...\n",
            "installing conda...\n",
            "installing amber...\n",
            "Downloading RNA3Db...\n",
            "Extracting structure files...\n",
            "Extracting sequence files...\n"
          ]
        }
      ],
      "source": [
        "#@title Install dependencies\n",
        "import os\n",
        "from google.colab import files\n",
        "import re\n",
        "import hashlib\n",
        "import random\n",
        "\n",
        "from sys import version_info\n",
        "python_version = f\"{version_info.major}.{version_info.minor}\"\n",
        "\n",
        "os.system(\"pip install biopython\")\n",
        "from Bio.PDB import *\n",
        "\n",
        "USE_AMBER = True\n",
        "USE_TEMPLATES = False\n",
        "PYTHON_VERSION = python_version\n",
        "\n",
        "if not os.path.isfile(\"COLABFOLD_READY\"):\n",
        "  print(\"installing colabfold...\")\n",
        "  os.system(\"pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'\")\n",
        "  if os.environ.get('TPU_NAME', False) != False:\n",
        "    os.system(\"pip uninstall -y jax jaxlib\")\n",
        "    os.system(\"pip install --no-warn-conflicts --upgrade dm-haiku==0.0.10 'jax[cuda12_pip]'==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\")\n",
        "  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold\")\n",
        "  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold\")\n",
        "  os.system(\"touch COLABFOLD_READY\")\n",
        "\n",
        "if USE_AMBER or USE_TEMPLATES:\n",
        "  if not os.path.isfile(\"CONDA_READY\"):\n",
        "    print(\"installing conda...\")\n",
        "    os.system(\"wget -qnc https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\")\n",
        "    os.system(\"bash Miniforge3-Linux-x86_64.sh -bfp /usr/local\")\n",
        "    os.system(\"mamba config --set auto_update_conda false\")\n",
        "    os.system(\"touch CONDA_READY\")\n",
        "\n",
        "if USE_TEMPLATES and not os.path.isfile(\"HH_READY\") and USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n",
        "  print(\"installing hhsuite and amber...\")\n",
        "  os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer\")\n",
        "  os.system(\"touch HH_READY\")\n",
        "  os.system(\"touch AMBER_READY\")\n",
        "else:\n",
        "  if USE_TEMPLATES and not os.path.isfile(\"HH_READY\"):\n",
        "    print(\"installing hhsuite...\")\n",
        "    os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python='{PYTHON_VERSION}'\")\n",
        "    os.system(\"touch HH_READY\")\n",
        "  if USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n",
        "    print(\"installing amber...\")\n",
        "    os.system(f\"mamba install -y -c conda-forge openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer\")\n",
        "    os.system(\"touch AMBER_READY\")\n",
        "\n",
        "if not os.path.exists(\"rna3db-mmcifs\"):\n",
        "  print(\"Downloading RNA3Db...\")\n",
        "  # Donwload RNA3Db structure files\n",
        "  os.system('wget https://github.com/marcellszi/rna3db/releases/download/incremental-update/rna3db-mmcifs.v2.tar.xz')\n",
        "  print(\"Extracting structure files...\")\n",
        "  os.system('sudo tar -xf rna3db-mmcifs.v2.tar.xz')\n",
        "  os.system('rm rna3db-mmcifs.v2.tar.xz')\n",
        "\n",
        "  # Donwload RNA3Db sequence files\n",
        "  os.system('wget https://github.com/marcellszi/rna3db/releases/download/incremental-update/rna3db-jsons.tar.gz')\n",
        "  print(\"Extracting sequence files...\")\n",
        "  os.system('tar -xzf rna3db-jsons.tar.gz')\n",
        "  os.system('rm rna3db-jsons.tar.gz')\n",
        "seq_path = \"/content/rna3db-jsons/cluster.json\"\n",
        "struct_path = \"/rna3db-mmcifs/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gvGvIauirXxE"
      },
      "outputs": [],
      "source": [
        "def add_hash(x,y):\n",
        "  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Eiq6hD4ArXxF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "class Converter(nn.Module):\n",
        "    def __init__(self, max_seq_len, d_model=64, nhead=8, num_layers=6, dim_feedforward=256, dropout=0.1):\n",
        "        super(Converter, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.input_embedding = nn.Linear(1, d_model)\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len=max_seq_len)\n",
        "\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
        "                                                    dim_feedforward=dim_feedforward,\n",
        "                                                    dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
        "\n",
        "        self.output_linear = nn.Linear(d_model, 20)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask=None):\n",
        "        # x shape: (seq_len, batch_size, 1)\n",
        "\n",
        "        x = self.input_embedding(x)  # Now: (seq_len, batch_size, d_model)\n",
        "\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        x = self.output_linear(x)  # Now: (seq_len, batch_size, 20)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        # Convert softmaxxed matrices into one-dimensional indeces\n",
        "        with torch.no_grad():\n",
        "            out = []\n",
        "            for i in range(len(x)):\n",
        "                out.append([])\n",
        "                for j in range(len(x[i])):\n",
        "                    out[-1].append((torch.argmax(x[i][j].detach().cpu())).item())\n",
        "        return out\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "def create_padding_mask(sequences, pad_value=0):\n",
        "    # sequences shape: (seq_len, batch_size, 1)\n",
        "    return (sequences.squeeze(-1) == pad_value).t()  # (batch_size, seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r4lGo43TrXxF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "def parse_json(path, a, b, max_len=150):\n",
        "    num = -1\n",
        "    seqs = {}\n",
        "    comps = []\n",
        "    macros = []\n",
        "    f = open(path)\n",
        "    data = json.load(f)\n",
        "    for i in data:\n",
        "        for j in data[i]:\n",
        "          for k in data[i][j]:\n",
        "            num = num + 1\n",
        "            if data[i][j][k][\"length\"]>max_len:\n",
        "                continue\n",
        "            if num>=a and num<=b:\n",
        "                seqs[k]=data[i][j][k][\"sequence\"]\n",
        "                comps.append(i)\n",
        "                macros.append(j)\n",
        "            if num>b:\n",
        "                break\n",
        "    f.close()\n",
        "    return seqs, comps, macros\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "w3f_KoWrrXxF"
      },
      "outputs": [],
      "source": [
        "seqs = {} # All sequences - may get quite large\n",
        "\n",
        "# Used for file tree searching\n",
        "components = []\n",
        "macro_tags = []\n",
        "\n",
        "\n",
        "# Index to amino acid dictionary\n",
        "# Largely arbitrary, but must stay consistent for any given converter\n",
        "AA_DICT = {\n",
        "    0: \"A\",\n",
        "    1: \"R\",\n",
        "    2: \"N\",\n",
        "    3: \"D\",\n",
        "    4: \"C\",\n",
        "    5: \"Q\",\n",
        "    6: \"E\",\n",
        "    7: \"G\",\n",
        "    8: \"H\",\n",
        "    9: \"I\",\n",
        "    10: \"L\",\n",
        "    11: \"K\",\n",
        "    12: \"M\",\n",
        "    13: \"F\",\n",
        "    14: \"P\",\n",
        "    15: \"S\",\n",
        "    16: \"T\",\n",
        "    17: \"W\",\n",
        "    18: \"Y\",\n",
        "    19: \"V\"\n",
        "}\n",
        "\n",
        "def load_data(path, a=0, b=float('inf'), max_len=150):\n",
        "    # Load up sequences, components, and macro-tags\n",
        "    seqs, components, macro_tags=parse_json(path, a, b, max_len=max_len)\n",
        "    print(f\"Found {len(seqs)} usable RNA strands...\")\n",
        "    return seqs, components, macro_tags\n",
        "\n",
        "def batch_data(iterable, n=1):\n",
        "    # Random data batching function\n",
        "    l = len(iterable)\n",
        "    iter = [(t, s) for t, s in list(iterable.items())]\n",
        "    for ndx in range(0, l, n):\n",
        "        yield iter[ndx:min(ndx + n, l)]\n",
        "\n",
        "def encode_rna(seq):\n",
        "    # Convert RNA sequence to nums to feed into Converter\n",
        "    out = []\n",
        "    for i in seq:\n",
        "        if i==\"A\":\n",
        "            out.append([0])\n",
        "        elif i==\"U\":\n",
        "            out.append([1])\n",
        "        elif i==\"C\":\n",
        "            out.append([2])\n",
        "        elif i==\"G\":\n",
        "            out.append([3])\n",
        "    return out\n",
        "\n",
        "def write_fastas(seqs):\n",
        "    # Write a dict of {tag: seq} to as many FASTA files as needed\n",
        "    os.makedirs('FASTAs', exist_ok=True)\n",
        "    for tag, seq in list(seqs.items()):\n",
        "        if os.path.exists(f'/content/FASTAs/{tag}.fasta'):\n",
        "            continue\n",
        "        f = open(f\"/content/FASTAs/{tag}.fasta\", \"w+\")\n",
        "        f.write(f\">{tag}\\n{seq}\")\n",
        "        f.close()\n",
        "\n",
        "def empty_dir(path):\n",
        "    # Empty any directory\n",
        "    for f in os.listdir(path):\n",
        "        if os.path.isfile(os.path.join(path, f)):\n",
        "          os.remove(os.path.join(path, f))\n",
        "        else:\n",
        "          empty_dir(os.path.join(path, f))\n",
        "    os.rmdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KYY8fVvarXxF"
      },
      "outputs": [],
      "source": [
        "def get_structure(tag, path):\n",
        "    # Return the structure of an RNA molecule given its tag and the path to the structure directory\n",
        "    # File directory:\n",
        "    # root\n",
        "    #  -- colabfold.dir\n",
        "    #  -- train_protify.ipynb\n",
        "    #  -- data.dir\n",
        "    #  ---- component 1.dir\n",
        "    #  ------ tag 1.dir\n",
        "    #  -------- tag 1a.cif\n",
        "    #  -------- tag 1b.cif\n",
        "    # ...\n",
        "    index = list(seqs.keys()).index(tag)\n",
        "    component = components[index]\n",
        "    macro_tag = macro_tags[index]\n",
        "\n",
        "    path = f\"/content/{path}/train_set/{component}/{macro_tag}/{tag}.cif\"\n",
        "    return path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hQbKsTuprXxF"
      },
      "outputs": [],
      "source": [
        "#@markdown ### Advanced settings\n",
        "model_type = \"auto\" #@param [\"auto\", \"alphafold2_ptm\", \"alphafold2_multimer_v1\", \"alphafold2_multimer_v2\", \"alphafold2_multimer_v3\", \"deepfold_v1\"]\n",
        "#@markdown - if `auto` selected, will use `alphafold2_ptm` for monomer prediction and `alphafold2_multimer_v3` for complex prediction.\n",
        "#@markdown Any of the mode_types can be used (regardless if input is monomer or complex).\n",
        "num_recycles = \"3\" #@param [\"auto\", \"0\", \"1\", \"3\", \"6\", \"12\", \"24\", \"48\"]\n",
        "#@markdown - if `auto` selected, will use `num_recycles=20` if `model_type=alphafold2_multimer_v3`, else `num_recycles=3` .\n",
        "recycle_early_stop_tolerance = \"auto\" #@param [\"auto\", \"0.0\", \"0.5\", \"1.0\"]\n",
        "#@markdown - if `auto` selected, will use `tol=0.5` if `model_type=alphafold2_multimer_v3` else `tol=0.0`.\n",
        "relax_max_iterations = 200 #@param [0, 200, 2000] {type:\"raw\"}\n",
        "#@markdown - max amber relax iterations, `0` = unlimited (AlphaFold2 default, can take very long)\n",
        "pairing_strategy = \"greedy\" #@param [\"greedy\", \"complete\"] {type:\"string\"}\n",
        "#@markdown - `greedy` = pair any taxonomically matching subsets, `complete` = all sequences have to match in one line.\n",
        "\n",
        "\n",
        "#@markdown #### Sample settings\n",
        "#@markdown -  enable dropouts and increase number of seeds to sample predictions from uncertainty of the model.\n",
        "#@markdown -  decrease `max_msa` to increase uncertainity\n",
        "max_msa = \"auto\" #@param [\"auto\", \"512:1024\", \"256:512\", \"64:128\", \"32:64\", \"16:32\"]\n",
        "num_seeds = 1 #@param [1,2,4,8,16] {type:\"raw\"}\n",
        "use_dropout = False #@param {type:\"boolean\"}\n",
        "\n",
        "num_recycles = None if num_recycles == \"auto\" else int(num_recycles)\n",
        "recycle_early_stop_tolerance = None if recycle_early_stop_tolerance == \"auto\" else float(recycle_early_stop_tolerance)\n",
        "if max_msa == \"auto\": max_msa = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "D6nW4eP-rXxG"
      },
      "outputs": [],
      "source": [
        "#@markdown ### MSA options (custom MSA upload, single sequence, pairing mode)\n",
        "msa_mode = \"mmseqs2_uniref_env\" #@param [\"mmseqs2_uniref_env\", \"mmseqs2_uniref\",\"single_sequence\",\"custom\"]\n",
        "pair_mode = \"unpaired_paired\" #@param [\"unpaired_paired\",\"paired\",\"unpaired\"] {type:\"string\"}\n",
        "#@markdown - \"unpaired_paired\" = pair sequences from same species + unpaired MSA, \"unpaired\" = seperate MSA for each chain, \"paired\" - only use paired sequences.\n",
        "\n",
        "def a3ms(jobname):\n",
        "    # decide which a3m to use\n",
        "    if \"mmseqs2\" in msa_mode:\n",
        "        a3m_file = os.path.join(jobname,f\"{jobname}.a3m\")\n",
        "\n",
        "    elif msa_mode == \"custom\":\n",
        "        a3m_file = os.path.join(jobname,f\"{jobname}.custom.a3m\")\n",
        "        if not os.path.isfile(a3m_file):\n",
        "            custom_msa_dict = files.upload()\n",
        "            custom_msa = list(custom_msa_dict.keys())[0]\n",
        "            header = 0\n",
        "            import fileinput\n",
        "            for line in fileinput.FileInput(custom_msa,inplace=1):\n",
        "                if line.startswith(\">\"):\n",
        "                    header = header + 1\n",
        "                if not line.rstrip():\n",
        "                    continue\n",
        "                if line.startswith(\">\") == False and header == 1:\n",
        "                    query_sequence = line.rstrip()\n",
        "                print(line, end='')\n",
        "\n",
        "            os.rename(custom_msa, a3m_file)\n",
        "            queries_path=a3m_file\n",
        "            print(f\"moving {custom_msa} to {a3m_file}\")\n",
        "\n",
        "    else:\n",
        "        a3m_file = os.path.join(jobname,f\"{jobname}.single_sequence.a3m\")\n",
        "        with open(a3m_file, \"w\") as text_file:\n",
        "            text_file.write(\">1\\n%s\" % query_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OrIHj5ExrXxG"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "class Monomer:\n",
        "    def __init__(self, macro):\n",
        "        self.atoms = dict()\n",
        "        self.name = \"\"\n",
        "        self.macro = macro\n",
        "\n",
        "    def add_atom(self, x, y, z, e):\n",
        "        self.atoms[e] = np.array([x, y, z])\n",
        "\n",
        "    def element(self, e):\n",
        "        if e[0]==\"\\\"\":\n",
        "            return e[1]\n",
        "        else:\n",
        "            return e[0]\n",
        "\n",
        "    def get_atoms(self):\n",
        "        return self.atoms\n",
        "\n",
        "    def add_name(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def apply_transformation(self, x, y, z):\n",
        "        out = self\n",
        "        for atom in out.atoms:\n",
        "            out.atoms[atom] += [x,y,z]\n",
        "        return out\n",
        "\n",
        "    def calculate_normal(self):\n",
        "        # Get the triangle vertices\n",
        "        c4_pos = np.array(self.atoms['\"C4\\'\"'])\n",
        "        c1_pos = np.array(self.atoms['\"C1\\'\"'])\n",
        "        translation = -np.array(self.atoms['P'])\n",
        "\n",
        "        # Calculate triangle vectors\n",
        "        p_to_c4 = c4_pos + translation  # Vector from P to C4'\n",
        "        p_to_c1 = c1_pos + translation  # Vector from P to C1'\n",
        "\n",
        "        # Calculate normal to triangle\n",
        "        normal = np.cross(p_to_c4, p_to_c1)\n",
        "        normal = normal / np.linalg.norm(normal)\n",
        "        return normal\n",
        "\n",
        "    def align_triangle_to_xy(self):\n",
        "        \"\"\"\n",
        "        Aligns the triangle formed by C4', C1', and N1/N9 atoms to the positive xy plane.\n",
        "        \"\"\"\n",
        "        out = copy.deepcopy(self)\n",
        "        # Get the coordinates of the three atoms forming the triangle\n",
        "        c4_coords = np.array(out.get_atom_coordinates('\\\"C4\\'\\\"'))\n",
        "        c1_coords = np.array(out.get_atom_coordinates('\\\"C1\\'\\\"'))\n",
        "        base_coords = np.array(out.get_atom_coordinates(\"P\"))\n",
        "\n",
        "        if c4_coords is None or c1_coords is None or base_coords is None:\n",
        "            raise ValueError(\"Could not find required atoms for alignment\")\n",
        "\n",
        "        # Create vectors from C4' to C1' and C4' to N1/N9\n",
        "        v1 = c1_coords - c4_coords\n",
        "        v2 = base_coords - c4_coords\n",
        "\n",
        "        # Calculate the normal vector of the triangle\n",
        "        normal = np.cross(v1, v2)\n",
        "        normal_magnitude = np.linalg.norm(normal)\n",
        "\n",
        "        if normal_magnitude < 1e-10:\n",
        "            raise ValueError(\"Colinear points cannot form a triangle\")\n",
        "\n",
        "        normal = normal / normal_magnitude\n",
        "\n",
        "        # Calculate rotation matrix to align normal vector with z-axis\n",
        "        z_axis = np.array([0, 0, 1])\n",
        "        rotation_axis = np.cross(normal, z_axis)\n",
        "        rotation_axis_magnitude = np.linalg.norm(rotation_axis)\n",
        "\n",
        "        if rotation_axis_magnitude < 1e-10:\n",
        "            # If vectors are parallel, no rotation needed or rotate 180° if antiparallel\n",
        "            if normal[2] < 0:\n",
        "                # If normal points in negative z, rotate 180° around x-axis\n",
        "                rotation_matrix = np.array([\n",
        "                    [1, 0, 0],\n",
        "                    [0, -1, 0],\n",
        "                    [0, 0, -1]\n",
        "                ])\n",
        "            else:\n",
        "                return  # Already aligned correctly\n",
        "        else:\n",
        "            rotation_axis = rotation_axis / rotation_axis_magnitude\n",
        "            angle = np.arccos(np.clip(np.dot(normal, z_axis), -1.0, 1.0))\n",
        "\n",
        "            # Create rotation matrix using Rodrigues' rotation formula\n",
        "            K = np.array([\n",
        "                [0, -rotation_axis[2], rotation_axis[1]],\n",
        "                [rotation_axis[2], 0, -rotation_axis[0]],\n",
        "                [-rotation_axis[1], rotation_axis[0], 0]\n",
        "            ])\n",
        "            rotation_matrix = (np.eye(3) + np.sin(angle) * K +\n",
        "                            (1 - np.cos(angle)) * np.matmul(K, K))\n",
        "\n",
        "        # Apply rotation to all atoms\n",
        "        for atom in out.atoms.keys():\n",
        "            coords = np.array(out.atoms[atom]) - c4_coords  # Center around C4'\n",
        "            rotated_coords = np.dot(rotation_matrix, coords)\n",
        "            atom.set_coordinates(rotated_coords + c4_coords)  # Move back to original position\n",
        "\n",
        "        # After first rotation, calculate the angle in xy plane between C4'-C1' vector and x-axis\n",
        "        c4_coords = np.array(out.get_atom_coordinates('C4\\''))\n",
        "        c1_coords = np.array(out.get_atom_coordinates('C1\\''))\n",
        "        v1_xy = c1_coords[:2] - c4_coords[:2]  # Only consider x and y components\n",
        "        v1_xy_magnitude = np.linalg.norm(v1_xy)\n",
        "\n",
        "        if v1_xy_magnitude < 1e-10:\n",
        "            return  # Vector is vertical, no need for xy rotation\n",
        "\n",
        "        cos_theta = np.clip(np.dot(v1_xy, [1, 0]) / v1_xy_magnitude, -1.0, 1.0)\n",
        "        theta = np.arccos(cos_theta)\n",
        "\n",
        "        # Determine if we need to rotate clockwise or counterclockwise\n",
        "        if v1_xy[1] < 0:\n",
        "            theta = -theta\n",
        "\n",
        "        # Create rotation matrix around z-axis\n",
        "        rotation_matrix_z = np.array([\n",
        "            [np.cos(theta), -np.sin(theta), 0],\n",
        "            [np.sin(theta), np.cos(theta), 0],\n",
        "            [0, 0, 1]\n",
        "        ])\n",
        "\n",
        "        # Apply second rotation to all atoms\n",
        "        for atom in out.atoms:\n",
        "            coords = np.array(atom.get_coordinates()) - c4_coords\n",
        "            rotated_coords = np.dot(rotation_matrix_z, coords)\n",
        "            atom.set_coordinates(rotated_coords + c4_coords)\n",
        "\n",
        "        # Final check to ensure the molecule is in the positive xy plane\n",
        "        # If the base atom is in the negative x region, rotate 180° around y-axis\n",
        "        base_coords = np.array(out.get_atom_coordinates(out.base_atom))\n",
        "        if base_coords[0] - c4_coords[0] < 0:\n",
        "            rotation_matrix_y = np.array([\n",
        "                [-1, 0, 0],\n",
        "                [0, 1, 0],\n",
        "                [0, 0, -1]\n",
        "            ])\n",
        "            for atom in out.atoms:\n",
        "                coords = np.array(atom.get_coordinates()) - c4_coords\n",
        "                rotated_coords = np.dot(rotation_matrix_y, coords)\n",
        "                atom.set_coordinates(rotated_coords + c4_coords)\n",
        "\n",
        "\n",
        "    def align_to_normal(self, target_normal):\n",
        "        \"\"\"\n",
        "        Rotates the monomer so that the normal vector of its P-C1'-C4' triangle\n",
        "        aligns with the given target normal vector.\n",
        "\n",
        "        Args:\n",
        "            target_normal (np.ndarray): The target normal vector to align with (should be normalized)\n",
        "\n",
        "        Returns:\n",
        "            Monomer: A new Monomer instance with the rotated coordinates\n",
        "        \"\"\"\n",
        "        out = copy.deepcopy(self)\n",
        "\n",
        "        try:\n",
        "            # Get current normal vector\n",
        "            current_normal = self.calculate_normal()\n",
        "\n",
        "            # Normalize target vector\n",
        "            target_normal = target_normal / np.linalg.norm(target_normal)\n",
        "\n",
        "            # Calculate rotation axis and angle\n",
        "            rotation_axis = np.cross(current_normal, target_normal)\n",
        "\n",
        "            # If vectors are parallel (or anti-parallel), rotation axis will be zero\n",
        "            if np.linalg.norm(rotation_axis) < 1e-10:\n",
        "                # If normals are anti-parallel, rotate 180° around any perpendicular axis\n",
        "                if np.dot(current_normal, target_normal) < 0:\n",
        "                    # Find a perpendicular vector to rotate around\n",
        "                    if abs(current_normal[0]) < abs(current_normal[1]):\n",
        "                        rotation_axis = np.cross(current_normal, [1, 0, 0])\n",
        "                    else:\n",
        "                        rotation_axis = np.cross(current_normal, [0, 1, 0])\n",
        "                    angle = np.pi\n",
        "                else:\n",
        "                    # Vectors are already aligned\n",
        "                    return out\n",
        "            else:\n",
        "                # Calculate rotation angle\n",
        "                angle = np.arccos(np.clip(np.dot(current_normal, target_normal), -1.0, 1.0))\n",
        "\n",
        "            # Normalize rotation axis\n",
        "            rotation_axis = rotation_axis / np.linalg.norm(rotation_axis)\n",
        "\n",
        "            # Create rotation matrix using Rodrigues' rotation formula\n",
        "            cos_theta = np.cos(angle)\n",
        "            sin_theta = np.sin(angle)\n",
        "            K = np.array([\n",
        "                [0, -rotation_axis[2], rotation_axis[1]],\n",
        "                [rotation_axis[2], 0, -rotation_axis[0]],\n",
        "                [-rotation_axis[1], rotation_axis[0], 0]\n",
        "            ])\n",
        "            R = np.eye(3) + sin_theta * K + (1 - cos_theta) * np.dot(K, K)\n",
        "\n",
        "            # Apply rotation to all atoms\n",
        "            for atom in out.atoms:\n",
        "                out.atoms[atom] = np.dot(R, out.atoms[atom])\n",
        "\n",
        "            return out\n",
        "\n",
        "        except KeyError as e:\n",
        "            raise KeyError(f\"Required atom {e} not found in this monomer\")\n",
        "\n",
        "\n",
        "    def load_template(self, n):\n",
        "        if n==\"A\": path = \"templates\\\\Adenine_template.cif\"\n",
        "        elif n==\"C\": path = \"templates\\\\Cytosine_template.cif\"\n",
        "        elif n==\"G\": path = \"templates\\\\Guanine_template.cif\"\n",
        "        elif n==\"U\": path = \"templates\\\\Uracil_template.cif\"\n",
        "        atoms = []\n",
        "        atom_xs = []\n",
        "        atom_ys = []\n",
        "        atom_zs = []\n",
        "        try:\n",
        "          parser = MMCIFParser()\n",
        "          structure = parser.get_structure(n, path)\n",
        "          data = []\n",
        "          for model in structure:\n",
        "            for chain in model:\n",
        "                for residue in chain:\n",
        "                    for atom in residue:\n",
        "                        datum = list(atom.get_vector())\n",
        "                        temp = (datum[0], datum[1], datum[2], atom.get_name())\n",
        "                        data.append(temp)\n",
        "        except Exception as e:\n",
        "            print(\"Oops. %s\" % e)\n",
        "        for i in range(len(data)):\n",
        "            self.add_atom(data[i])\n",
        "\n",
        "\n",
        "    def __str__(self, start=1):\n",
        "        #Return what this monomer would look like in an mmCIF file#\n",
        "        out = \"\"\n",
        "        c = start\n",
        "        for i in self.atoms:\n",
        "            out += f\"ATOM {c}\\t{self.element(i)}\\t{i}\\t{self.name}\\t. A 1 1\\t?\\t{round(self.atoms[i][0],3)}\\t{round(self.atoms[i][1],3)}\\t{round(self.atoms[i][2],3)}\\n\"\n",
        "            c += 1\n",
        "        out += \"\\b\\b\"\n",
        "        return out\n",
        "\n",
        "    def print(self, start=1, number=1):\n",
        "        #Return what this monomer would look like in an mmCIF file#\n",
        "        out = \"\"\n",
        "        c = start\n",
        "        for i in self.atoms:\n",
        "            out += f\"ATOM {c}\\t{self.element(i)}\\t{i}\\t{self.name}\\t. A 1 {number}\\t?\\t{round(self.atoms[i][0],3)}\\t{round(self.atoms[i][1],3)}\\t{round(self.atoms[i][2],3)}\\n\"\n",
        "            c += 1\n",
        "        return out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.atoms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wu_t90EurXxG"
      },
      "outputs": [],
      "source": [
        "def RMSD(p1, p2):\n",
        "    loss = torch.sqrt(torch.mean((p1[:len(p2)] - p2)**2))\n",
        "    return loss\n",
        "\n",
        "def tm_score(p1, p2, lt):\n",
        "    d0 = lambda l: 1.24 * torch.power(l-15, 3) - 1.8\n",
        "    loss = torch.mean(1/(1+torch.power(torch.abs(torch.norm(p1-p2))/d0(lt),2)))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CKJCFsdFrXxH"
      },
      "outputs": [],
      "source": [
        "def parse_rna(path):\n",
        "    try:\n",
        "        parser = MMCIFParser()\n",
        "        structure = parser.get_structure(\"RNA\", path)\n",
        "        data = []\n",
        "        for model in structure:\n",
        "          for chain in model:\n",
        "              for residue in chain:\n",
        "                  for atom in residue:\n",
        "                    if residue.get_resname() in ['A', 'U', 'C', 'G']:\n",
        "                      datum = list(atom.get_vector())\n",
        "                      temp = (datum[0], datum[1], datum[2], atom.get_name())\n",
        "                      data.append(temp)\n",
        "\n",
        "        points = []\n",
        "        angle_points = []\n",
        "        norms = []\n",
        "\n",
        "        for x, y, z, atom in data:\n",
        "            x = float(x)\n",
        "            y = float(y)\n",
        "            z = float(z)\n",
        "\n",
        "            if atom == \"P\":\n",
        "                points.append(np.array([x, y, z]))\n",
        "                angle_points.append(np.array([x, y, z]))\n",
        "            elif atom == \"\\\"C1'\\\"\":\n",
        "                angle_points.append(np.array([x, y, z]))\n",
        "            elif atom == \"\\\"C4'\\\"\":\n",
        "                angle_points.append(np.array([x, y, z]))\n",
        "                v1 = angle_points[-1]-angle_points[-2]\n",
        "                v2 = angle_points[-3]-angle_points[-2]\n",
        "                norms.append(np.cross(v1, v2))\n",
        "                angle_points = []\n",
        "        points = np.array(points)\n",
        "        norms = np.array(norms)\n",
        "        return torch.tensor(points, requires_grad=True, dtype=torch.float32), torch.tensor(norms, requires_grad=True, dtype=torch.float32)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Oops. %s\" % e)\n",
        "        sys.exit(1)\n",
        "\n",
        "def parse_protein(path):\n",
        "    try:\n",
        "        parser = PDBParser()\n",
        "        structure = parser.get_structure(\"Protein\", path)\n",
        "        data = []\n",
        "        for model in structure:\n",
        "          for chain in model:\n",
        "              for residue in chain:\n",
        "                  for atom in residue:\n",
        "                      datum = list(atom.get_vector())\n",
        "                      temp = (datum[0], datum[1], datum[2], atom.get_name())\n",
        "                      data.append(temp)\n",
        "\n",
        "        points = []\n",
        "        angle_points = []\n",
        "        norms = []\n",
        "        for x, y, z, atom in data:\n",
        "            x = float(x)\n",
        "            y = float(y)\n",
        "            z = float(z)\n",
        "\n",
        "            if atom == \"CA\":\n",
        "                points.append(np.array([x, y, z]))\n",
        "                angle_points.append(np.array([x, y, z]))\n",
        "            elif atom == \"N\":\n",
        "                angle_points.append(np.array([x, y, z]))\n",
        "            elif atom == \"C\":\n",
        "                angle_points.append(np.array([x, y, z]))\n",
        "                v1 = angle_points[-1]-angle_points[-2]\n",
        "                v2 = angle_points[-3]-angle_points[-2]\n",
        "                norms.append(np.cross(v1, v2))\n",
        "                angle_points = []\n",
        "\n",
        "        points = np.array(points)\n",
        "        norms = np.array(norms)\n",
        "        return torch.tensor(points, requires_grad=True), torch.tensor(norms, requires_grad=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Oops. %s\" % e)\n",
        "        sys.exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "I-79HfcXrXxH"
      },
      "outputs": [],
      "source": [
        "def protein_to_rna(protein, rna_path, tm=False):\n",
        "    prot_points, _ = parse_protein(protein)\n",
        "    rna_points, _ = parse_rna(rna_path)\n",
        "    prot_points = correct_protein_coords(prot_points)\n",
        "    if tm:\n",
        "        return tm_score(prot_points, rna_points)\n",
        "    return RMSD(prot_points, rna_points)\n",
        "\n",
        "def correct_protein_coords(points):\n",
        "  # Apply correction factor to the protein coordinates to account for bond lengths\n",
        "  correction_factor = torch.zeros(3, dtype=torch.float32, requires_grad=True) # Delta correction factor\n",
        "  pp_dist = 6.8 # Approximated this value from what ChatGPT tells me - will look for rigorous results\n",
        "                # Also haha pp\n",
        "  # Create a copy of the points to avoid in-place modification\n",
        "  new_points = points.clone()\n",
        "  for i in range(1,len(points)):\n",
        "    v = points[i]-points[i-1] # Vector between two points\n",
        "    pt = pp_dist*v/torch.norm(v) # Vector to new point\n",
        "    correction_factor = correction_factor + pt-v\n",
        "    # Update the copy instead of the original points\n",
        "    new_points[i] = points[i]+correction_factor # Apply correction factor\n",
        "  return new_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IC0NUKBVrXxH"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "from Bio import BiopythonDeprecationWarning\n",
        "warnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)\n",
        "from pathlib import Path\n",
        "from colabfold.download import download_alphafold_params, default_data_dir\n",
        "from colabfold.utils import setup_logging\n",
        "from colabfold.batch import get_queries, run, set_model_type\n",
        "from colabfold.plot import plot_msa_v2\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "def input_features_callback(input_features):\n",
        "  pass\n",
        "\n",
        "def prediction_callback(protein_obj, length,\n",
        "                        prediction_result, input_features, mode):\n",
        "  model_name, relaxed = mode\n",
        "  pass\n",
        "\n",
        "def train(seqs, epochs=50, batch_size=32,tm_score=False, max_seq_len=150):\n",
        "    drive.mount('/content/drive')\n",
        "    !mkdir -p \"/content/drive/My Drive/ConverterWeights\"\n",
        "    try:\n",
        "        K80_chk = os.popen('nvidia-smi | grep \"Tesla K80\" | wc -l').read()\n",
        "    except:\n",
        "        K80_chk = \"0\"\n",
        "        pass\n",
        "    if \"1\" in K80_chk:\n",
        "        print(\"WARNING: found GPU Tesla K80: limited to total length < 1000\")\n",
        "        if \"TF_FORCE_UNIFIED_MEMORY\" in os.environ:\n",
        "            del os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]\n",
        "        if \"XLA_PYTHON_CLIENT_MEM_FRACTION\" in os.environ:\n",
        "            del os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]\n",
        "\n",
        "    # For some reason we need that to get pdbfixer to import\n",
        "    if f\"/usr/local/lib/python{python_version}/site-packages/\" not in sys.path:\n",
        "        sys.path.insert(0, f\"/usr/local/lib/python{python_version}/site-packages/\")\n",
        "\n",
        "    conv = Converter(max_seq_len=max_seq_len)\n",
        "    conv.train()\n",
        "    optimizer = torch.optim.Adam(conv.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "    model_type = set_model_type(False, \"auto\")\n",
        "    download_alphafold_params(model_type, Path(\".\"))\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for batch in batch_data(seqs, batch_size):\n",
        "            optimizer.zero_grad()\n",
        "            # batch: ([(tag, seq), (tag, seq),...])\n",
        "\n",
        "            # LAYER 1: RNA-AMINO CONVERSION\n",
        "            tags = [s[0] for s in batch]\n",
        "\n",
        "            # Check that structure files exist\n",
        "            # if not os.path.isfile(get_structure(tags[0])):\n",
        "            #     continue\n",
        "\n",
        "            # Preprocessing sequences\n",
        "            processed_seqs = [torch.tensor(np.transpose(np.array(encode_rna(s[1])), (0,1)), requires_grad=False, dtype=torch.float32) for s in batch] # (batch, seq, base)\n",
        "\n",
        "            # Send sequences through the converter\n",
        "            aa_seqs = [conv(s) for s in processed_seqs][0] # (seq, batch, aa)\n",
        "            temp = []\n",
        "\n",
        "            # Reconvert to letter representation\n",
        "            for i in range(len(aa_seqs)):\n",
        "                temp.append(''.join([AA_DICT[n] for n in aa_seqs[i]]))\n",
        "\n",
        "            aa_seqs = temp # (seq: String, batch)\n",
        "\n",
        "            final_seqs = {} # {tag: seq}\n",
        "            for i in range(len(tags)):\n",
        "                final_seqs[tags[i]] = aa_seqs[i]\n",
        "            write_fastas(final_seqs)\n",
        "\n",
        "            num_relax = 0 #@param [0, 1, 5] {type:\"raw\"}\n",
        "            #@markdown - specify how many of the top ranked structures to relax using amber\n",
        "            template_mode = \"none\" #@param [\"none\", \"pdb100\",\"custom\"]\n",
        "            #@markdown - `none` = no template information is used. `pdb100` = detect templates in pdb100 (see [notes](#pdb100)). `custom` - upload and search own templates (PDB or mmCIF format, see [notes](#custom_templates))\n",
        "\n",
        "            use_amber = num_relax > 0\n",
        "            use_cluster_profile = True\n",
        "\n",
        "            if template_mode == \"pdb100\":\n",
        "                use_templates = True\n",
        "                custom_template_path = None\n",
        "            elif template_mode == \"custom\":\n",
        "                custom_template_path = os.path.join(jobname,f\"template\")\n",
        "                os.makedirs(custom_template_path, exist_ok=True)\n",
        "                uploaded = files.upload()\n",
        "                use_templates = True\n",
        "                for fn in uploaded.keys():\n",
        "                    os.rename(fn,os.path.join(custom_template_path,fn))\n",
        "            else:\n",
        "                custom_template_path = None\n",
        "                use_templates = False\n",
        "\n",
        "            loss = torch.tensor([], dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "            for i in range(len(final_seqs)):\n",
        "                queries, _ = get_queries(f'/content/FASTAs/{list(final_seqs.keys())[i]}.fasta')\n",
        "                jobname = add_hash(list(final_seqs.keys())[i], list(final_seqs.values())[i])\n",
        "                results =  run(\n",
        "                    queries=queries,\n",
        "                    result_dir=jobname,\n",
        "                    use_templates=USE_TEMPLATES,\n",
        "                    custom_template_path=None,\n",
        "                    num_relax=num_relax,\n",
        "                    msa_mode=msa_mode,\n",
        "                    model_type=model_type,\n",
        "                    num_models=1,\n",
        "                    num_recycles=num_recycles,\n",
        "                    relax_max_iterations=relax_max_iterations,\n",
        "                    recycle_early_stop_tolerance=recycle_early_stop_tolerance,\n",
        "                    num_seeds=num_seeds,\n",
        "                    use_dropout=use_dropout,\n",
        "                    model_order=[1,2,3,4,5],\n",
        "                    is_complex=False,\n",
        "                    data_dir=Path(\".\"),\n",
        "                    keep_existing_results=False,\n",
        "                    rank_by=\"auto\",\n",
        "                    pair_mode=pair_mode,\n",
        "                    pairing_strategy=pairing_strategy,\n",
        "                    stop_at_score=float(100),\n",
        "                    prediction_callback=prediction_callback,\n",
        "                    dpi=200,\n",
        "                    zip_results=False,\n",
        "                    save_all=False,\n",
        "                    max_msa=max_msa,\n",
        "                    use_cluster_profile=use_cluster_profile,\n",
        "                    input_features_callback=input_features_callback,\n",
        "                    save_recycles=False,\n",
        "                    user_agent=\"colabfold/google-colab-main\",\n",
        "                )\n",
        "                path = \"\"\n",
        "                for file in os.listdir(f\"/content/{jobname}\"):\n",
        "                  if file.endswith(\".pdb\"):\n",
        "                    path = os.path.join(f\"/content/{jobname}\", file)\n",
        "                    break\n",
        "                temp_loss = (protein_to_rna(path, get_structure(list(final_seqs.keys())[i], struct_path), tm=tm_score))\n",
        "                epoch_loss+=temp_loss\n",
        "                empty_dir(f\"/content/{jobname}\")\n",
        "                torch.cat((loss, torch.tensor([temp_loss], dtype=torch.float32)), 0)\n",
        "\n",
        "            loss = torch.mean(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch}: Loss {epoch_loss/len(seqs)}\")\n",
        "        torch.save(conv, f'/content/drive/My Drive/ConverterWeights/converter_epoch_{epoch}.pt')\n",
        "        torch.save(conv.state_dict(), f'/content/drive/My Drive/ConverterWeights/converter_params_epoch_{epoch}.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kgfbtiMbrXxH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d186974-4077-4d6a-ccca-3d340e685b9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10 usable RNA strands...\n",
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n",
            "Downloading alphafold2_ptm weights to .: 100%|██████████| 3.47G/3.47G [00:34<00:00, 109MB/s]\n",
            "PENDING:   0%|          | 0/150 [elapsed: 00:00 remaining: ?]ERROR:colabfold.colabfold:Sleeping for 7s. Reason: PENDING\n",
            "COMPLETE: 100%|██████████| 150/150 [elapsed: 00:08 remaining: 00:00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(187.6265, dtype=torch.float64, grad_fn=<SqrtBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PENDING:   0%|          | 0/150 [elapsed: 00:00 remaining: ?]ERROR:colabfold.colabfold:Sleeping for 10s. Reason: PENDING\n",
            "COMPLETE: 100%|██████████| 150/150 [elapsed: 00:11 remaining: 00:00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(200.3732, dtype=torch.float64, grad_fn=<SqrtBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PENDING:   0%|          | 0/150 [elapsed: 00:00 remaining: ?]ERROR:colabfold.colabfold:Sleeping for 9s. Reason: PENDING\n",
            "COMPLETE: 100%|██████████| 150/150 [elapsed: 00:10 remaining: 00:00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(105.2179, dtype=torch.float64, grad_fn=<SqrtBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PENDING:   0%|          | 0/150 [elapsed: 00:00 remaining: ?]ERROR:colabfold.colabfold:Sleeping for 5s. Reason: PENDING\n",
            "COMPLETE: 100%|██████████| 150/150 [elapsed: 00:06 remaining: 00:00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(198.2539, dtype=torch.float64, grad_fn=<SqrtBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PENDING:   0%|          | 0/150 [elapsed: 00:00 remaining: ?]ERROR:colabfold.colabfold:Sleeping for 6s. Reason: PENDING\n",
            "COMPLETE: 100%|██████████| 150/150 [elapsed: 00:07 remaining: 00:00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(205.4314, dtype=torch.float64, grad_fn=<SqrtBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PENDING:   0%|          | 0/150 [elapsed: 00:00 remaining: ?]ERROR:colabfold.colabfold:Sleeping for 9s. Reason: PENDING\n",
            "COMPLETE: 100%|██████████| 150/150 [elapsed: 00:10 remaining: 00:00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(179.1743, dtype=torch.float64, grad_fn=<SqrtBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PENDING:   0%|          | 0/150 [elapsed: 00:00 remaining: ?]ERROR:colabfold.colabfold:Sleeping for 9s. Reason: PENDING\n",
            "COMPLETE: 100%|██████████| 150/150 [elapsed: 00:10 remaining: 00:00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(45.4130, dtype=torch.float64, grad_fn=<SqrtBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PENDING:   0%|          | 0/150 [elapsed: 00:00 remaining: ?]ERROR:colabfold.colabfold:Sleeping for 7s. Reason: PENDING\n",
            "COMPLETE: 100%|██████████| 150/150 [elapsed: 00:08 remaining: 00:00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(39.4401, dtype=torch.float64, grad_fn=<SqrtBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PENDING:   0%|          | 0/150 [elapsed: 00:00 remaining: ?]ERROR:colabfold.colabfold:Sleeping for 6s. Reason: PENDING\n",
            "COMPLETE: 100%|██████████| 150/150 [elapsed: 00:07 remaining: 00:00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(44.0015, dtype=torch.float64, grad_fn=<SqrtBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PENDING:   0%|          | 0/150 [elapsed: 00:00 remaining: ?]ERROR:colabfold.colabfold:Sleeping for 7s. Reason: PENDING\n",
            "COMPLETE: 100%|██████████| 150/150 [elapsed: 00:08 remaining: 00:00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(59.2969, dtype=torch.float64, grad_fn=<SqrtBackward0>)\n",
            "Epoch 1: Loss nan\n"
          ]
        }
      ],
      "source": [
        "seqs, components, macro_tags = load_data(seq_path, 0, 10)\n",
        "train(seqs, epochs=1, batch_size=3, max_seq_len=200)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}