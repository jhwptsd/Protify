{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_0pUGU74rXxD",
    "outputId": "6c87d46d-3384-44ff-eaf5-1bf1462a6db4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing colabfold...\n",
      "installing conda...\n",
      "installing amber...\n",
      "Downloading RNA3Db...\n",
      "Extracting structure files...\n",
      "Extracting sequence files...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import files\n",
    "import re\n",
    "import hashlib\n",
    "import random\n",
    "\n",
    "from sys import version_info\n",
    "python_version = f\"{version_info.major}.{version_info.minor}\"\n",
    "\n",
    "os.system(\"pip install biopython\")\n",
    "from Bio.PDB import *\n",
    "\n",
    "USE_AMBER = True\n",
    "USE_TEMPLATES = False\n",
    "PYTHON_VERSION = python_version\n",
    "\n",
    "if not os.path.isfile(\"COLABFOLD_READY\"):\n",
    "  print(\"installing colabfold...\")\n",
    "  os.system(\"pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'\")\n",
    "  if os.environ.get('TPU_NAME', False) != False:\n",
    "    os.system(\"pip uninstall -y jax jaxlib\")\n",
    "    os.system(\"pip install --no-warn-conflicts --upgrade dm-haiku==0.0.10 'jax[cuda12_pip]'==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\")\n",
    "  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold\")\n",
    "  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold\")\n",
    "  os.system(\"touch COLABFOLD_READY\")\n",
    "\n",
    "if USE_AMBER or USE_TEMPLATES:\n",
    "  if not os.path.isfile(\"CONDA_READY\"):\n",
    "    print(\"installing conda...\")\n",
    "    os.system(\"wget -qnc https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\")\n",
    "    os.system(\"bash Miniforge3-Linux-x86_64.sh -bfp /usr/local\")\n",
    "    os.system(\"mamba config --set auto_update_conda false\")\n",
    "    os.system(\"touch CONDA_READY\")\n",
    "\n",
    "if USE_TEMPLATES and not os.path.isfile(\"HH_READY\") and USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n",
    "  print(\"installing hhsuite and amber...\")\n",
    "  os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer\")\n",
    "  os.system(\"touch HH_READY\")\n",
    "  os.system(\"touch AMBER_READY\")\n",
    "else:\n",
    "  if USE_TEMPLATES and not os.path.isfile(\"HH_READY\"):\n",
    "    print(\"installing hhsuite...\")\n",
    "    os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python='{PYTHON_VERSION}'\")\n",
    "    os.system(\"touch HH_READY\")\n",
    "  if USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n",
    "    print(\"installing amber...\")\n",
    "    os.system(f\"mamba install -y -c conda-forge openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer\")\n",
    "    os.system(\"touch AMBER_READY\")\n",
    "\n",
    "if not os.path.exists(\"rna3db-mmcifs\"):\n",
    "  print(\"Downloading RNA3Db...\")\n",
    "  # Donwload RNA3Db structure files\n",
    "  os.system('wget https://github.com/marcellszi/rna3db/releases/download/incremental-update/rna3db-mmcifs.v2.tar.xz')\n",
    "  print(\"Extracting structure files...\")\n",
    "  os.system('sudo tar -xf rna3db-mmcifs.v2.tar.xz')\n",
    "  os.system('rm rna3db-mmcifs.v2.tar.xz')\n",
    "\n",
    "  # Donwload RNA3Db sequence files\n",
    "  os.system('wget https://github.com/marcellszi/rna3db/releases/download/incremental-update/rna3db-jsons.tar.gz')\n",
    "  print(\"Extracting sequence files...\")\n",
    "  os.system('tar -xzf rna3db-jsons.tar.gz')\n",
    "  os.system('rm rna3db-jsons.tar.gz')\n",
    "seq_path = \"/content/rna3db-jsons/cluster.json\"\n",
    "struct_path = \"/rna3db-mmcifs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gvGvIauirXxE"
   },
   "outputs": [],
   "source": [
    "def add_hash(x,y):\n",
    "  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converter Class\n",
    "This class is essentially just a Transformer network. Input: list of list of ints, ex [[0], [3], [1],...].\n",
    "Possible future routes of inquiry: Mamba modifications to the Transformer, other types of RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Eiq6hD4ArXxF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class Converter(nn.Module):\n",
    "    def __init__(self, max_seq_len=150, d_model=64, nhead=8, num_layers=6, dim_feedforward=256, dropout=0.1):\n",
    "        super(Converter, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.input_embedding = nn.Linear(1, d_model)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len=max_seq_len)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model=d_model,\n",
    "                                    nhead=nhead,\n",
    "                                    dim_feedforward=dim_feedforward,\n",
    "                                    num_encoder_layers=num_layers, num_decoder_layers=num_layers)\n",
    "\n",
    "        self.output_linear = nn.Linear(d_model, 20)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        # x shape: (seq_len, batch_size, 1)\n",
    "\n",
    "        x = self.input_embedding(x)  # Now: (seq_len, batch_size, d_model)\n",
    "\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        x = self.transformer(x, x, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        x = self.output_linear(x)  # Now: (seq_len, batch_size, 20)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        # Convert softmaxxed matrices into one-dimensional indeces\n",
    "        with torch.no_grad():\n",
    "            out = []\n",
    "            for i in range(len(x)):\n",
    "                out.append([])\n",
    "                for j in range(len(x[i])):\n",
    "                    out[-1].append((torch.argmax(x[i][j].detach().cpu())).item())\n",
    "        return out\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def create_padding_mask(sequences, pad_value=0):\n",
    "    # sequences shape: (seq_len, batch_size, 1)\n",
    "    return (sequences.squeeze(-1) == pad_value).t()  # (batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse json files for sequence information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "r4lGo43TrXxF"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "def parse_json(path, a, b, max_len=150):\n",
    "    num = -1\n",
    "    seqs = {}\n",
    "    comps = []\n",
    "    macros = []\n",
    "    f = open(path)\n",
    "    data = json.load(f)\n",
    "    for i in data:\n",
    "        for j in data[i]:\n",
    "          for k in data[i][j]:\n",
    "            num = num + 1\n",
    "            if data[i][j][k][\"length\"]>max_len:\n",
    "                continue\n",
    "            if num>=a and num<=b:\n",
    "                seqs[k]=data[i][j][k][\"sequence\"]\n",
    "                comps.append(i)\n",
    "                macros.append(j)\n",
    "            if num>b:\n",
    "                break\n",
    "    f.close()\n",
    "    return seqs, comps, macros\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "w3f_KoWrrXxF"
   },
   "outputs": [],
   "source": [
    "seqs = {} # All sequences - may get quite large\n",
    "\n",
    "# Used for file tree searching\n",
    "components = []\n",
    "macro_tags = []\n",
    "\n",
    "\n",
    "# Index to amino acid dictionary\n",
    "# Largely arbitrary, but must stay consistent for any given converter\n",
    "AA_DICT = {\n",
    "    0: \"A\",\n",
    "    1: \"R\",\n",
    "    2: \"N\",\n",
    "    3: \"D\",\n",
    "    4: \"C\",\n",
    "    5: \"Q\",\n",
    "    6: \"E\",\n",
    "    7: \"G\",\n",
    "    8: \"H\",\n",
    "    9: \"I\",\n",
    "    10: \"L\",\n",
    "    11: \"K\",\n",
    "    12: \"M\",\n",
    "    13: \"F\",\n",
    "    14: \"P\",\n",
    "    15: \"S\",\n",
    "    16: \"T\",\n",
    "    17: \"W\",\n",
    "    18: \"Y\",\n",
    "    19: \"V\"\n",
    "}\n",
    "\n",
    "def load_data(path, a=0, b=float('inf'), max_len=150):\n",
    "    # Load up sequences, components, and macro-tags\n",
    "    seqs, components, macro_tags=parse_json(path, a, b, max_len=max_len)\n",
    "    print(f\"Found {len(seqs)} usable RNA strands...\")\n",
    "    return seqs, components, macro_tags\n",
    "\n",
    "def batch_data(iterable, n=1):\n",
    "    # Random data batching function\n",
    "    l = len(iterable)\n",
    "    iter = [(t, s) for t, s in list(iterable.items())]\n",
    "    random.shuffle(iter)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iter[ndx:min(ndx + n, l)]\n",
    "\n",
    "def encode_rna(seq):\n",
    "    # Convert RNA sequence to nums to feed into Converter\n",
    "    out = []\n",
    "    for i in seq:\n",
    "        if i==\"A\":\n",
    "            out.append([0])\n",
    "        elif i==\"U\":\n",
    "            out.append([1])\n",
    "        elif i==\"C\":\n",
    "            out.append([2])\n",
    "        elif i==\"G\":\n",
    "            out.append([3])\n",
    "    return out\n",
    "\n",
    "def write_fastas(seqs):\n",
    "    # Write a dict of {tag: seq} to as many FASTA files as needed\n",
    "    os.makedirs('FASTAs', exist_ok=True)\n",
    "    for tag, seq in list(seqs.items()):\n",
    "        if os.path.exists(f'/content/FASTAs/{tag}.fasta'):\n",
    "            continue\n",
    "        f = open(f\"/content/FASTAs/{tag}.fasta\", \"w+\")\n",
    "        f.write(f\">{tag}\\n{seq}\")\n",
    "        f.close()\n",
    "\n",
    "def empty_dir(path, delete=True):\n",
    "    # Empty any directory\n",
    "    for f in os.listdir(path):\n",
    "        if os.path.isfile(os.path.join(path, f)):\n",
    "          os.remove(os.path.join(path, f))\n",
    "        else:\n",
    "          empty_dir(os.path.join(path, f))\n",
    "    if delete:\n",
    "      os.rmdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search the RNA3Db filetree for structure mmcifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KYY8fVvarXxF"
   },
   "outputs": [],
   "source": [
    "def get_structure(tag, path):\n",
    "    # Return the structure of an RNA molecule given its tag and the path to the structure directory\n",
    "    # File directory:\n",
    "    # root\n",
    "    #  -- colabfold.dir\n",
    "    #  -- train_protify.ipynb\n",
    "    #  -- data.dir\n",
    "    #  ---- component 1.dir\n",
    "    #  ------ tag 1.dir\n",
    "    #  -------- tag 1a.cif\n",
    "    #  -------- tag 1b.cif\n",
    "    # ...\n",
    "    index = list(seqs.keys()).index(tag)\n",
    "    component = components[index]\n",
    "    macro_tag = macro_tags[index]\n",
    "\n",
    "    path = f\"/content/{path}/train_set/{component}/{macro_tag}/{tag}.cif\"\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQbKsTuprXxF"
   },
   "outputs": [],
   "source": [
    "model_type = \"auto\"\n",
    "num_recycles = \"3\"\n",
    "recycle_early_stop_tolerance = \"auto\"\n",
    "relax_max_iterations = 200 \n",
    "pairing_strategy = \"greedy\"\n",
    "\n",
    "max_msa = \"auto\" #@param [\"auto\", \"512:1024\", \"256:512\", \"64:128\", \"32:64\", \"16:32\"]\n",
    "num_seeds = 1 #@param [1,2,4,8,16] {type:\"raw\"}\n",
    "use_dropout = False #@param {type:\"boolean\"}\n",
    "\n",
    "num_recycles = None if num_recycles == \"auto\" else int(num_recycles)\n",
    "recycle_early_stop_tolerance = None if recycle_early_stop_tolerance == \"auto\" else float(recycle_early_stop_tolerance)\n",
    "if max_msa == \"auto\": max_msa = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSA options\n",
    "For now, keep as single_sequence - otherwise, the API will be flooded with requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6nW4eP-rXxG"
   },
   "outputs": [],
   "source": [
    "msa_mode = \"single_sequence\" #@param [\"mmseqs2_uniref_env\", \"mmseqs2_uniref\",\"single_sequence\",\"custom\"]\n",
    "pair_mode = \"unpaired_paired\" #@param [\"unpaired_paired\",\"paired\",\"unpaired\"] {type:\"string\"}\n",
    "#@markdown - \"unpaired_paired\" = pair sequences from same species + unpaired MSA, \"unpaired\" = seperate MSA for each chain, \"paired\" - only use paired sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wu_t90EurXxG"
   },
   "outputs": [],
   "source": [
    "def RMSD(p1, p2):\n",
    "    if len(p1)>len(p2):\n",
    "      loss = torch.sqrt(torch.mean((p1[:len(p2)] - p2)**2))\n",
    "    else:\n",
    "      loss = torch.sqrt(torch.mean((p1 - p2[:len(p1)])**2))\n",
    "    return loss\n",
    "\n",
    "def tm_score(p1, p2, lt):\n",
    "    d0 = lambda l: 1.24 * torch.power(l-15, 3) - 1.8\n",
    "    loss = torch.mean(1/(1+torch.power(torch.abs(torch.norm(p1-p2))/d0(lt),2)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNA and Protein MMCIF/PDB Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "CKJCFsdFrXxH"
   },
   "outputs": [],
   "source": [
    "def parse_rna(path):\n",
    "    try:\n",
    "        parser = MMCIFParser()\n",
    "        structure = parser.get_structure(\"RNA\", path)\n",
    "        data = []\n",
    "        for model in structure:\n",
    "          for chain in model:\n",
    "              for residue in chain:\n",
    "                  for atom in residue:\n",
    "                    if residue.get_resname() in ['A', 'U', 'C', 'G']:\n",
    "                      datum = list(atom.get_vector())\n",
    "                      temp = (datum[0], datum[1], datum[2], atom.get_name())\n",
    "                      data.append(temp)\n",
    "\n",
    "        points = []\n",
    "        angle_points = []\n",
    "        norms = []\n",
    "\n",
    "        correction_factor = torch.zeros(3, dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "        for x, y, z, atom in data:\n",
    "            x = float(x)\n",
    "            y = float(y)\n",
    "            z = float(z)\n",
    "\n",
    "            point = np.add(np.array([x,y,z]), correction_factor)\n",
    "\n",
    "            if atom == \"P\":\n",
    "              if (correction_factor==torch.zeros(3)).all():\n",
    "                correction_factor = torch.tensor([-x, -y, -z])\n",
    "              points.append(point)\n",
    "              angle_points.append(point)\n",
    "            elif atom == \"\\\"C1'\\\"\":\n",
    "                angle_points.append(point)\n",
    "            elif atom == \"\\\"C4'\\\"\":\n",
    "                angle_points.append(point)\n",
    "                v1 = angle_points[-1]-angle_points[-2]\n",
    "                v2 = angle_points[-3]-angle_points[-2]\n",
    "                norms.append(np.cross(v1, v2))\n",
    "                angle_points = []\n",
    "        points = np.array(points)\n",
    "        norms = np.array(norms)\n",
    "        return torch.tensor(points, requires_grad=True, dtype=torch.float32), torch.tensor(norms, requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Oops. %s\" % e)\n",
    "        sys.exit(1)\n",
    "\n",
    "def parse_protein(path):\n",
    "    try:\n",
    "        parser = PDBParser()\n",
    "        structure = parser.get_structure(\"Protein\", path)\n",
    "        data = []\n",
    "        for model in structure:\n",
    "          for chain in model:\n",
    "              for residue in chain:\n",
    "                  for atom in residue:\n",
    "                      datum = list(atom.get_vector())\n",
    "                      temp = (datum[0], datum[1], datum[2], atom.get_name())\n",
    "                      data.append(temp)\n",
    "\n",
    "        points = []\n",
    "        angle_points = []\n",
    "        norms = []\n",
    "\n",
    "        correction_factor = torch.zeros(3, dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "        for x, y, z, atom in data:\n",
    "            x = float(x)\n",
    "            y = float(y)\n",
    "            z = float(z)\n",
    "\n",
    "            point = np.add(np.array([x,y,z]), correction_factor)\n",
    "            if atom == \"CA\":\n",
    "              if (correction_factor==torch.zeros(3)).all():\n",
    "                correction_factor = torch.tensor([-x, -y, -z])\n",
    "              points.append(point)\n",
    "              angle_points.append(point)\n",
    "            elif atom == \"N\":\n",
    "                angle_points.append(point)\n",
    "            elif atom == \"C\":\n",
    "                angle_points.append(point)\n",
    "                v1 = angle_points[-1]-angle_points[-2]\n",
    "                v2 = angle_points[-3]-angle_points[-2]\n",
    "                norms.append(np.cross(v1, v2))\n",
    "                angle_points = []\n",
    "\n",
    "        points = np.array(points)\n",
    "        norms = np.array(norms)\n",
    "        return torch.tensor(points, requires_grad=True), torch.tensor(norms, requires_grad=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Oops. %s\" % e)\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNA/Protein Comparison function + Protein coordinate correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "I-79HfcXrXxH"
   },
   "outputs": [],
   "source": [
    "def protein_to_rna(protein, rna_path, corrector, tm=False):\n",
    "    prot_points, _ = parse_protein(protein)\n",
    "    rna_points, _ = parse_rna(rna_path)\n",
    "    prot_points = correct_protein_coords(prot_points, corrector)\n",
    "    if tm:\n",
    "        return tm_score(prot_points, rna_points)\n",
    "    return RMSD(prot_points, rna_points)\n",
    "\n",
    "def correct_protein_coords(points, corrector):\n",
    "    correction_factor = corrector.unsqueeze(0)\n",
    "\n",
    "    # Calculate vector differences between consecutive points\n",
    "    vectors = points[1:] - points[:-1]\n",
    "    norms = torch.norm(vectors, dim=1, keepdim=True)\n",
    "    normalized_vectors = vectors / norms\n",
    "\n",
    "    # Apply correction factor\n",
    "    corrected_vectors = normalized_vectors * correction_factor\n",
    "\n",
    "    corrected_points = torch.zeros_like(points)\n",
    "    corrected_points[0] = points[0]\n",
    "    corrected_points[1:] = points[:-1] + corrected_vectors\n",
    "\n",
    "    return corrected_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IC0NUKBVrXxH"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from Bio import BiopythonDeprecationWarning\n",
    "warnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)\n",
    "from pathlib import Path\n",
    "from colabfold.download import download_alphafold_params, default_data_dir\n",
    "from colabfold.utils import setup_logging\n",
    "from colabfold.batch import get_queries, run, set_model_type\n",
    "from colabfold.plot import plot_msa_v2\n",
    "import os\n",
    "from google.colab import drive\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "def input_features_callback(input_features):\n",
    "  pass\n",
    "\n",
    "def prediction_callback(protein_obj, length,\n",
    "                        prediction_result, input_features, mode):\n",
    "  model_name, relaxed = mode\n",
    "  pass\n",
    "\n",
    "def train(seqs, epochs=50, batch_size=32,tm_score=False, max_seq_len=150, converter=None, pp_dist=6.8):\n",
    "    !mkdir -p \"/content/drive/My Drive/ConverterWeights\"\n",
    "    try:\n",
    "        K80_chk = os.popen('nvidia-smi | grep \"Tesla K80\" | wc -l').read()\n",
    "    except:\n",
    "        K80_chk = \"0\"\n",
    "        pass\n",
    "    if \"1\" in K80_chk:\n",
    "        print(\"WARNING: found GPU Tesla K80: limited to total length < 1000\")\n",
    "        if \"TF_FORCE_UNIFIED_MEMORY\" in os.environ:\n",
    "            del os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]\n",
    "        if \"XLA_PYTHON_CLIENT_MEM_FRACTION\" in os.environ:\n",
    "            del os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]\n",
    "\n",
    "    # For some reason we need that to get pdbfixer to import\n",
    "    if f\"/usr/local/lib/python{python_version}/site-packages/\" not in sys.path:\n",
    "        sys.path.insert(0, f\"/usr/local/lib/python{python_version}/site-packages/\")\n",
    "\n",
    "    if converter==None:\n",
    "      conv = Converter(max_seq_len=max_seq_len)\n",
    "    else:\n",
    "      conv = converter\n",
    "    conv.train()\n",
    "    corrector = [nn.Parameter(torch.tensor(pp_dist, requires_grad=True, dtype=torch.float32))] # Can't be bothered to do research, so I'll just regress it\n",
    "    optimizer = torch.optim.AdamW(conv.parameters(), lr=1e-2)\n",
    "    dist_optimizer = torch.optim.AdamW(corrector, lr=1e-2)\n",
    "\n",
    "    model_type = set_model_type(False, \"auto\")\n",
    "    download_alphafold_params(model_type, Path(\".\"))\n",
    "    for epoch in range(epochs):\n",
    "        for batch in batch_data(seqs, batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            dist_optimizer.zero_grad()\n",
    "            # batch: ([(tag, seq), (tag, seq),...])\n",
    "\n",
    "            # LAYER 1: RNA-AMINO CONVERSION\n",
    "            tags = [s[0] for s in batch]\n",
    "\n",
    "            # Preprocessing sequences\n",
    "            processed_seqs = [torch.tensor(np.transpose(np.array(encode_rna(s[1])), (0,1)), requires_grad=False, dtype=torch.float32) for s in batch] # (batch, seq, base)\n",
    "\n",
    "            # Send sequences through the converter\n",
    "            aa_seqs = [conv(s) for s in processed_seqs][0] # (seq, batch, aa)\n",
    "            temp = []\n",
    "\n",
    "            # Reconvert to letter representation\n",
    "            for i in range(len(aa_seqs)):\n",
    "                temp.append(''.join([AA_DICT[n] for n in aa_seqs[i]]))\n",
    "\n",
    "            aa_seqs = temp # (seq: String, batch)\n",
    "\n",
    "            final_seqs = {} # {tag: seq}\n",
    "            for i in range(len(tags)):\n",
    "                final_seqs[tags[i]] = aa_seqs[i]\n",
    "            write_fastas(final_seqs)\n",
    "\n",
    "            num_relax = 1 #@param [0, 1, 5] {type:\"raw\"}\n",
    "            #@markdown - specify how many of the top ranked structures to relax using amber\n",
    "            template_mode = \"none\" #@param [\"none\", \"pdb100\",\"custom\"]\n",
    "            #@markdown - `none` = no template information is used. `pdb100` = detect templates in pdb100 (see [notes](#pdb100)). `custom` - upload and search own templates (PDB or mmCIF format, see [notes](#custom_templates))\n",
    "\n",
    "            use_amber = num_relax > 0\n",
    "            use_cluster_profile = True\n",
    "\n",
    "            if template_mode == \"pdb100\":\n",
    "                use_templates = True\n",
    "                custom_template_path = None\n",
    "            elif template_mode == \"custom\":\n",
    "                custom_template_path = os.path.join(jobname,f\"template\")\n",
    "                os.makedirs(custom_template_path, exist_ok=True)\n",
    "                uploaded = files.upload()\n",
    "                use_templates = True\n",
    "                for fn in uploaded.keys():\n",
    "                    os.rename(fn,os.path.join(custom_template_path,fn))\n",
    "            else:\n",
    "                custom_template_path = None\n",
    "                use_templates = False\n",
    "\n",
    "            loss = []\n",
    "            lengths = 0\n",
    "\n",
    "            for i in tqdm(range(len(final_seqs))):\n",
    "              lengths=lengths+list(final_seqs.values())[i]\n",
    "              with torch.no_grad():\n",
    "                queries, _ = get_queries(f'/content/FASTAs/{list(final_seqs.keys())[i]}.fasta')\n",
    "                jobname = add_hash(list(final_seqs.keys())[i], list(final_seqs.values())[i])\n",
    "                results = run(\n",
    "                    queries=queries,\n",
    "                    result_dir=jobname,\n",
    "                    use_templates=USE_TEMPLATES,\n",
    "                    custom_template_path=None,\n",
    "                    num_relax=num_relax,\n",
    "                    msa_mode=msa_mode,\n",
    "                    model_type=model_type,\n",
    "                    num_models=1,\n",
    "                    num_recycles=num_recycles,\n",
    "                    relax_max_iterations=relax_max_iterations,\n",
    "                    recycle_early_stop_tolerance=recycle_early_stop_tolerance,\n",
    "                    num_seeds=num_seeds,\n",
    "                    use_dropout=use_dropout,\n",
    "                    model_order=[1,2,3,4,5],\n",
    "                    is_complex=False,\n",
    "                    data_dir=Path(\".\"),\n",
    "                    keep_existing_results=False,\n",
    "                    rank_by=\"auto\",\n",
    "                    pair_mode=pair_mode,\n",
    "                    pairing_strategy=pairing_strategy,\n",
    "                    stop_at_score=float(100),\n",
    "                    prediction_callback=prediction_callback,\n",
    "                    dpi=200,\n",
    "                    zip_results=False,\n",
    "                    save_all=False,\n",
    "                    max_msa=max_msa,\n",
    "                    use_cluster_profile=use_cluster_profile,\n",
    "                    input_features_callback=input_features_callback,\n",
    "                    save_recycles=False,\n",
    "                    user_agent=\"colabfold/google-colab-main\",\n",
    "                )\n",
    "                path = \"\"\n",
    "                for file in os.listdir(f\"/content/{jobname}\"):\n",
    "                  if file.endswith(\".pdb\"):\n",
    "                    path = os.path.join(f\"/content/{jobname}\", file)\n",
    "                    break\n",
    "              temp_loss = (protein_to_rna(path, get_structure(list(final_seqs.keys())[i], struct_path), corrector[0], tm=tm_score))\n",
    "\n",
    "                # Download generated/actual for qualitative comp\n",
    "                # shutil.copy(path, \"/content/generated.pdb\")\n",
    "                # shutil.copy(get_structure(list(final_seqs.keys())[i], struct_path), \"/content/actual.cif\")\n",
    "              with torch.no_grad():\n",
    "                loss.append(temp_loss)\n",
    "                empty_dir(f\"/content/{jobname}\")\n",
    "\n",
    "            lengths = lengths/batch_size\n",
    "\n",
    "            empty_dir(\"FASTAs\", delete=False)\n",
    "            loss = torch.mean(torch.stack(loss))\n",
    "            print(f\"\\n\\nCurrent Loss: {loss}\")\n",
    "            print(f\"Average Loss per Residue: {loss/lengths}\")\n",
    "            print(f\"Correction factor: {corrector}\\n\\n\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            dist_optimizer.step()\n",
    "        torch.save(conv, f'/content/drive/My Drive/ConverterWeights/converter_epoch_{epoch}.pt')\n",
    "        torch.save(conv.state_dict(), f'/content/drive/My Drive/ConverterWeights/converter_params_epoch_{epoch}.pt')\n",
    "        torch.save(corrector, f'/content/drive/My Drive/ConverterWeights/corrector_epoch_{epoch}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kgfbtiMbrXxH",
    "outputId": "e2366a49-58bf-4365-87b3-1175171c19b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 695 usable RNA strands...\n",
      "Mounted at /content/drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "Downloading alphafold2_ptm weights to .: 100%|██████████| 3.47G/3.47G [02:39<00:00, 23.4MB/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]WARNING:root:Warning: importing 'simtk.openmm' is deprecated.  Import 'openmm' instead.\n",
      "100%|██████████| 4/4 [02:33<00:00, 38.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 47.17895296083071\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.8000, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:16<00:00, 34.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 39.04995134218149\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.7893, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:54<00:00, 28.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 48.61932220953027\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.7786, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:37<00:00, 39.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 50.61705792991525\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.7680, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:43<00:00, 40.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 42.66410344176859\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.7576, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:54<00:00, 43.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 35.5614499047273\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.7471, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:27<00:00, 36.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 46.01149324720523\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.7365, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:20<00:00, 35.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 40.22920174075317\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.7259, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:28<00:00, 37.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 49.02489395287125\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.7152, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:51<00:00, 27.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 44.27498864615886\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.7045, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:59<00:00, 29.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 39.82487112158808\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.6942, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:00<00:00, 30.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 47.878161987285296\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.6842, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:07<00:00, 31.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 44.06607922199578\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.6742, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:28<00:00, 37.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 47.1476896014951\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.6642, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:12<00:00, 33.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 50.90345010544664\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.6542, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:34<00:00, 38.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 44.39965157721785\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.6444, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:03<00:00, 30.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 38.2050279641886\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.6344, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:11<00:00, 32.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 44.27729844124853\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.6244, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:07<00:00, 31.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 36.69503401567894\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.6148, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:30<00:00, 37.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 41.74346670041062\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.6054, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:08<00:00, 32.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 53.983660381045155\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.5959, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:11<00:00, 32.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 42.837888798814625\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.5864, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:15<00:00, 33.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 41.895656433131904\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.5770, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:31<00:00, 37.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 42.96268997115444\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.5670, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:12<00:00, 33.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 37.856721998004524\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.5569, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:13<00:00, 33.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 39.87874548816668\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.5466, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:57<00:00, 29.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 47.45169509096484\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.5365, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:04<00:00, 31.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 45.73169420760474\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.5260, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:55<00:00, 28.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 45.389442360893156\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.5156, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:57<00:00, 29.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 39.33420107457361\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.5054, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:12<00:00, 33.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 43.11103125082031\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.4957, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:55<00:00, 28.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 41.005447382473676\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.4858, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:03<00:00, 30.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 55.548762357178994\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.4758, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:00<00:00, 30.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 38.887389934746764\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.4655, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:19<00:00, 34.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 31.290606322672332\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.4549, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:18<00:00, 34.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 57.378527914526806\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.4441, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:51<00:00, 27.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 43.25209638923713\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.4331, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:00<00:00, 30.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 46.466522060725325\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.4223, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:23<00:00, 35.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 50.35959143724527\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.4113, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [03:17<00:00, 49.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 40.593850687496975\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.4007, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:16<00:00, 34.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 42.792528917373616\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.3900, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:10<00:00, 32.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 49.845047082073904\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.3797, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:58<00:00, 29.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 43.30407825552454\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.3689, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:03<00:00, 30.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Loss: 43.82769929886865\n",
      "Correction factor: [Parameter containing:\n",
      "tensor(6.3584, requires_grad=True)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a45053563918>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#   c.load_state_dict(ckpt['model_state_dict'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#   print(\"Loaded model parameters from checkpoint\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, pp_dist=float(corrector[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-de0480265a2f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(seqs, epochs, batch_size, tm_score, max_seq_len, converter, pp_dist)\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_queries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/content/FASTAs/{list(final_seqs.keys())[i]}.fasta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mjobname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_seqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_seqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 results =  run(\n\u001b[0m\u001b[1;32m    109\u001b[0m                     \u001b[0mqueries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0mresult_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjobname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/colabfold/batch.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(queries, result_dir, num_models, is_complex, num_recycles, recycle_early_stop_tolerance, model_order, num_ensemble, model_type, msa_mode, use_templates, custom_template_path, num_relax, relax_max_iterations, relax_tolerance, relax_stiffness, relax_max_outer_iterations, keep_existing_results, rank_by, pair_mode, pairing_strategy, data_dir, host_url, user_agent, random_seed, num_seeds, recompile_padding, zip_results, prediction_callback, save_single_representations, save_pair_representations, jobname_prefix, save_all, save_recycles, use_dropout, use_gpu_relax, stop_at_score, dpi, max_seq, max_extra_seq, pdb_hit_file, local_pdb_path, use_cluster_profile, feature_dict_callback, **kwargs)\u001b[0m\n\u001b[1;32m   1558\u001b[0m                         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Setting max_seq={max_seq}, max_extra_seq={max_extra_seq}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1560\u001b[0;31m                     model_runner_and_params = load_models_and_params(\n\u001b[0m\u001b[1;32m   1561\u001b[0m                         \u001b[0mnum_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m                         \u001b[0muse_templates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_templates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/colabfold/alphafold/models.py\u001b[0m in \u001b[0;36mload_models_and_params\u001b[0;34m(num_models, use_templates, num_recycles, recycle_early_stop_tolerance, num_ensemble, model_order, model_type, data_dir, stop_at_score, rank_by, max_seq, max_extra_seq, use_cluster_profile, use_fuse, use_bfloat16, use_dropout, save_all)\u001b[0m\n\u001b[1;32m    170\u001b[0m             )\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         params = get_model_haiku_params(\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mmodel_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_number\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/colabfold/alphafold/models.py\u001b[0m in \u001b[0;36mget_model_haiku_params\u001b[0;34m(data_dir, model_type, model_number, use_fuse, to_jnp)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"params\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_params_to_haiku\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_fuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_jnp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_jnp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/alphafold/model/utils.py\u001b[0m in \u001b[0;36mflat_params_to_haiku\u001b[0;34m(params, fuse, to_jnp)\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Convert a dictionary of NumPy arrays to Haiku parameters.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscope\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/_collections_abc.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    909\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmagic\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAGIC_PREFIX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 return format.read_array(bytes,\n\u001b[0m\u001b[1;32m    257\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                                          \u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    829\u001b[0m                     \u001b[0mread_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_read_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                     \u001b[0mread_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_count\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m                     array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[1;32m    833\u001b[0m                                                              count=read_count)\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;31m# done about that.  note that regular files can't be non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_left\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_crc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_update_crc\u001b[0;34m(self, newdata)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0;31m# No need to compute the CRC if we don't have a reference value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrc32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m         \u001b[0;31m# Check the CRC if we're at the end of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_running_crc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expected_crc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seqs, components, macro_tags = load_data(seq_path, 0, 1615, max_len=100)\n",
    "drive.mount('/content/drive')\n",
    "c = Converter(max_seq_len=200)\n",
    "corrector = [nn.Parameter(torch.tensor(0, requires_grad=True, dtype=torch.float32))]\n",
    "# try:\n",
    "#   c = torch.load('/content/drive/My Drive/ConverterWeights/converter.pt')\n",
    "#   corrector = torch.load('/content/drive/My Drive/ConverterWeights/corrector.pt')\n",
    "#   print(\"Loaded model from checkpoint\")\n",
    "# except:\n",
    "#   c = Converter(max_seq_len=200)\n",
    "#   ckpt = torch.load('/content/drive/My Drive/ConverterWeights/converter_params.pt', weights_only=True)\n",
    "#   corrector = corrector = torch.load('/content/drive/My Drive/ConverterWeights/corrector.pt')\n",
    "#   c.load_state_dict(ckpt['model_state_dict'])\n",
    "#   print(\"Loaded model parameters from checkpoint\")\n",
    "train(seqs, epochs=1, batch_size=4, max_seq_len=100, converter=c)#, pp_dist=float(corrector[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model (params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "q-KQX4bhqfWV"
   },
   "outputs": [],
   "source": [
    "torch.save(c, f'/content/drive/My Drive/ConverterWeights/converter.pt')\n",
    "torch.save(c.state_dict(), f'/content/drive/My Drive/ConverterWeights/converter_params.pt')\n",
    "torch.save(corrector, f'/content/drive/My Drive/ConverterWeights/corrector.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model on dummy sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "in2gzt14WuGx",
    "outputId": "0d73d58d-80c0-4462-981e-8a4866ddb5b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPGPPDNPHGHHHPPH\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "s = \"AUGCGGGAAAAAUUCG\"\n",
    "processed_seq = torch.tensor(np.transpose(np.array(encode_rna(s)), (0,1)), requires_grad=False, dtype=torch.float32) # (batch, seq, base)\n",
    "# Send sequences through the converter\n",
    "aa_seqs = c(processed_seq)[0] # (seq, batch, aa)\n",
    "temp = []\n",
    "\n",
    "# Reconvert to letter representation\n",
    "temp.append(''.join([AA_DICT[n] for n in aa_seqs]))\n",
    "\n",
    "aa_seqs = temp # (seq: String, batch)\n",
    "print(aa_seqs[0])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
